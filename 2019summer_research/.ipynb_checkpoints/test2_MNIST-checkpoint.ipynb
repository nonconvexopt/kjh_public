{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir(\"/juhyeong/projects/2019연구학점제/\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random as rd\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../../data/MNIST/', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../../data/MNIST/', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])), batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)\n",
    "    \n",
    "class flatten_concat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flatten_concat, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return torch.mean(x.view(-1, shape), 0).unsqueeze(0)\n",
    "    \n",
    "class sampler_net(nn.Module):\n",
    "    def __init__(self, num_params):\n",
    "        super(sampler_net, self).__init__()\n",
    "        self.kernel_size = (3, 3)\n",
    "        self.num_params = num_params\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1, 4, kernel_size = self.kernel_size, padding = tuple(x//2 for x in self.kernel_size), stride = (2, 2)),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            \n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, kernel_size = self.kernel_size, padding = tuple(x//2 for x in self.kernel_size), stride = (2, 2)),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "\n",
    "            flatten_concat()\n",
    "        )\n",
    "        \n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(32, self.num_params),\n",
    "        )\n",
    "        \n",
    "        self.std = nn.Sequential(\n",
    "            nn.Linear(32, self.num_params),\n",
    "        )\n",
    "\n",
    "        self.sample_dist = torch.distributions.MultivariateNormal(torch.zeros(self.num_params), torch.eye(self.num_params))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        logvar = self.std(x)\n",
    "        return (mean + (self.sample_dist.sample().cuda() * (0.5 * logvar).exp())).squeeze(), mean, logvar\n",
    "\n",
    "    \n",
    "class sample_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sample_net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size = (3, 3), padding = (1, 1), stride = (2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            \n",
    "            nn.Conv2d(4, 8, kernel_size = (3, 3), padding = (1, 1), stride = (2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            \n",
    "            flatten(),\n",
    "            \n",
    "            nn.Linear(2 * 2 * 8, 10, bias = True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        params = params\n",
    "        param_count = 0\n",
    "        self.layers[0].weight = torch.nn.Parameter(params[param_count:param_count + self.layers[0].weight.numel()].view(self.layers[0].weight.size()))\n",
    "        param_count += self.layers[0].weight.numel()\n",
    "        self.layers[0].bias = torch.nn.Parameter(params[param_count:param_count + self.layers[0].bias.numel()].view(self.layers[0].bias.size()))\n",
    "        param_count += self.layers[0].bias.numel()\n",
    "        self.layers[3].weight = torch.nn.Parameter(params[param_count:param_count + self.layers[3].weight.numel()].view(self.layers[3].weight.size()))\n",
    "        param_count += self.layers[3].weight.numel()\n",
    "        self.layers[3].bias = torch.nn.Parameter(params[param_count:param_count + self.layers[3].bias.numel()].view(self.layers[3].bias.size()))\n",
    "        param_count += self.layers[3].bias.numel()\n",
    "        self.layers[7].weight = torch.nn.Parameter(params[param_count:param_count + self.layers[7].weight.numel()].view(self.layers[7].weight.size()))\n",
    "        param_count += self.layers[7].weight.numel()\n",
    "        self.layers[7].bias = torch.nn.Parameter(params[param_count:param_count + self.layers[7].bias.numel()].view(self.layers[7].bias.size()))\n",
    "        param_count += self.layers[7].bias.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 4, 288, 8, 320, 10]\n",
      "666\n",
      "[1, 1, 36, 4, 4, 4, 288, 8, 21312, 666, 21312, 666]\n",
      "44302\n"
     ]
    }
   ],
   "source": [
    "sample = sample_net().cuda()\n",
    "print([p.numel() for p in sample.parameters() if p.requires_grad])\n",
    "print(sum(p.numel() for p in sample.parameters() if p.requires_grad))\n",
    "\n",
    "sampler = sampler_net(sum(p.numel() for p in sample.parameters() if p.requires_grad)).cuda()\n",
    "print([p.numel() for p in sampler.parameters() if p.requires_grad])\n",
    "print(sum(p.numel() for p in sampler.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "lr = 3e-4\n",
    "optimizer_sample = torch.optim.Adam(sample.parameters(), lr = lr)\n",
    "optimizer_sampler = torch.optim.Adam(sampler.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_net().cuda()\n",
    "sampler = sampler_net(sum(p.numel() for p in sample.parameters() if p.requires_grad)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_loss = nn.CrossEntropyLoss()\n",
    "def multi_crit(output, label, mean, logvar):\n",
    "    loss = 0.0\n",
    "    loss += CE_loss(output, label)\n",
    "    loss += -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return loss\n",
    "criterion = multi_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0,train loss = 0.67529, test loss = 0.70752, running test loss = 0.70752, time: 24.09 sec\n",
      "epoch : 1,train loss = 0.72354, test loss = 0.71731, running test loss = 0.71241, time: 24.66 sec\n",
      "epoch : 2,train loss = 0.72267, test loss = 0.79448, running test loss = 0.73977, time: 21.79 sec\n",
      "epoch : 3,train loss = 0.65827, test loss = 0.68218, running test loss = 0.72537, time: 18.63 sec\n",
      "epoch : 4,train loss = 0.71045, test loss = 0.73679, running test loss = 0.72766, time: 18.66 sec\n",
      "epoch : 5,train loss = 0.71502, test loss = 0.71827, running test loss = 0.72609, time: 15.68 sec\n",
      "epoch : 6,train loss = 0.69270, test loss = 0.65385, running test loss = 0.71577, time: 15.67 sec\n",
      "epoch : 7,train loss = 0.69350, test loss = 0.68593, running test loss = 0.71204, time: 21.51 sec\n",
      "epoch : 8,train loss = 0.71191, test loss = 0.81930, running test loss = 0.72396, time: 24.58 sec\n",
      "epoch : 9,train loss = 0.67993, test loss = 0.64129, running test loss = 0.71569, time: 24.15 sec\n",
      "epoch : 10,train loss = 0.71381, test loss = 0.60812, running test loss = 0.70591, time: 24.27 sec\n",
      "epoch : 11,train loss = 0.66502, test loss = 0.67849, running test loss = 0.70363, time: 24.39 sec\n",
      "epoch : 12,train loss = 0.68245, test loss = 0.67785, running test loss = 0.70164, time: 24.70 sec\n",
      "epoch : 13,train loss = 0.67838, test loss = 0.65936, running test loss = 0.69862, time: 24.53 sec\n",
      "epoch : 14,train loss = 0.68974, test loss = 0.67678, running test loss = 0.69717, time: 24.22 sec\n",
      "epoch : 15,train loss = 0.67744, test loss = 0.75838, running test loss = 0.70099, time: 24.55 sec\n",
      "epoch : 16,train loss = 0.71123, test loss = 0.73768, running test loss = 0.70315, time: 23.35 sec\n",
      "epoch : 17,train loss = 0.70382, test loss = 0.70466, running test loss = 0.70324, time: 24.25 sec\n",
      "epoch : 18,train loss = 0.68766, test loss = 0.63339, running test loss = 0.69956, time: 24.20 sec\n",
      "epoch : 19,train loss = 0.73050, test loss = 0.74917, running test loss = 0.70204, time: 24.54 sec\n",
      "epoch : 20,train loss = 0.71847, test loss = 0.73022, running test loss = 0.70338, time: 24.41 sec\n",
      "epoch : 21,train loss = 0.69493, test loss = 0.69284, running test loss = 0.70290, time: 24.58 sec\n",
      "epoch : 22,train loss = 0.67851, test loss = 0.63342, running test loss = 0.69988, time: 24.02 sec\n",
      "epoch : 23,train loss = 0.66222, test loss = 0.73356, running test loss = 0.70129, time: 24.42 sec\n",
      "epoch : 24,train loss = 0.69053, test loss = 0.67850, running test loss = 0.70037, time: 24.49 sec\n",
      "epoch : 25,train loss = 0.67823, test loss = 0.63489, running test loss = 0.69786, time: 23.91 sec\n",
      "epoch : 26,train loss = 0.69448, test loss = 0.72559, running test loss = 0.69888, time: 24.50 sec\n",
      "epoch : 27,train loss = 0.69393, test loss = 0.76114, running test loss = 0.70111, time: 24.37 sec\n",
      "epoch : 28,train loss = 0.68774, test loss = 0.69267, running test loss = 0.70082, time: 24.63 sec\n",
      "epoch : 29,train loss = 0.67781, test loss = 0.66931, running test loss = 0.69977, time: 24.32 sec\n",
      "epoch : 30,train loss = 0.69836, test loss = 0.69728, running test loss = 0.69969, time: 23.83 sec\n",
      "epoch : 31,train loss = 0.68863, test loss = 0.76117, running test loss = 0.70161, time: 24.22 sec\n",
      "epoch : 32,train loss = 0.71026, test loss = 0.84229, running test loss = 0.70587, time: 24.29 sec\n",
      "epoch : 33,train loss = 0.70894, test loss = 0.63682, running test loss = 0.70384, time: 23.62 sec\n",
      "epoch : 34,train loss = 0.69157, test loss = 0.71374, running test loss = 0.70412, time: 24.79 sec\n",
      "epoch : 35,train loss = 0.70079, test loss = 0.67120, running test loss = 0.70321, time: 24.47 sec\n",
      "epoch : 36,train loss = 0.69373, test loss = 0.71503, running test loss = 0.70353, time: 24.29 sec\n",
      "epoch : 37,train loss = 0.71417, test loss = 0.75921, running test loss = 0.70499, time: 24.44 sec\n",
      "epoch : 38,train loss = 0.67991, test loss = 0.71606, running test loss = 0.70528, time: 24.50 sec\n",
      "epoch : 39,train loss = 0.74163, test loss = 0.71451, running test loss = 0.70551, time: 24.30 sec\n",
      "epoch : 40,train loss = 0.71487, test loss = 0.75025, running test loss = 0.70660, time: 23.62 sec\n",
      "epoch : 41,train loss = 0.71042, test loss = 0.71651, running test loss = 0.70683, time: 24.86 sec\n",
      "epoch : 42,train loss = 0.67747, test loss = 0.70016, running test loss = 0.70668, time: 24.53 sec\n",
      "epoch : 43,train loss = 0.70654, test loss = 0.71839, running test loss = 0.70694, time: 24.47 sec\n",
      "epoch : 44,train loss = 0.72605, test loss = 0.64344, running test loss = 0.70553, time: 24.18 sec\n",
      "epoch : 45,train loss = 0.71382, test loss = 0.59781, running test loss = 0.70319, time: 23.10 sec\n",
      "epoch : 46,train loss = 0.67556, test loss = 0.72955, running test loss = 0.70375, time: 24.19 sec\n",
      "epoch : 47,train loss = 0.66232, test loss = 0.62141, running test loss = 0.70204, time: 24.09 sec\n",
      "epoch : 48,train loss = 0.69852, test loss = 0.65376, running test loss = 0.70105, time: 24.28 sec\n",
      "epoch : 49,train loss = 0.69235, test loss = 0.73444, running test loss = 0.70172, time: 23.90 sec\n",
      "epoch : 50,train loss = 0.73489, test loss = 0.66982, running test loss = 0.70109, time: 24.03 sec\n",
      "epoch : 51,train loss = 0.66987, test loss = 0.72464, running test loss = 0.70155, time: 24.49 sec\n",
      "epoch : 52,train loss = 0.69396, test loss = 0.83600, running test loss = 0.70408, time: 24.19 sec\n",
      "epoch : 53,train loss = 0.69666, test loss = 0.62840, running test loss = 0.70268, time: 24.12 sec\n",
      "epoch : 54,train loss = 0.72256, test loss = 0.67370, running test loss = 0.70216, time: 24.43 sec\n",
      "epoch : 55,train loss = 0.71497, test loss = 0.69590, running test loss = 0.70204, time: 23.86 sec\n",
      "epoch : 56,train loss = 0.73253, test loss = 0.72785, running test loss = 0.70250, time: 24.43 sec\n",
      "epoch : 57,train loss = 0.69273, test loss = 0.68757, running test loss = 0.70224, time: 23.57 sec\n",
      "epoch : 58,train loss = 0.66670, test loss = 0.80756, running test loss = 0.70402, time: 23.91 sec\n",
      "epoch : 59,train loss = 0.69643, test loss = 0.64458, running test loss = 0.70303, time: 24.00 sec\n",
      "epoch : 60,train loss = 0.71002, test loss = 0.74471, running test loss = 0.70372, time: 24.12 sec\n",
      "epoch : 61,train loss = 0.67984, test loss = 0.62575, running test loss = 0.70246, time: 24.16 sec\n",
      "epoch : 62,train loss = 0.71643, test loss = 0.76089, running test loss = 0.70339, time: 24.10 sec\n",
      "epoch : 63,train loss = 0.68561, test loss = 0.64644, running test loss = 0.70250, time: 23.88 sec\n",
      "epoch : 64,train loss = 0.70198, test loss = 0.76823, running test loss = 0.70351, time: 24.33 sec\n",
      "epoch : 65,train loss = 0.71849, test loss = 0.67709, running test loss = 0.70311, time: 24.20 sec\n",
      "epoch : 66,train loss = 0.70190, test loss = 0.69388, running test loss = 0.70297, time: 24.35 sec\n",
      "epoch : 67,train loss = 0.71316, test loss = 0.73236, running test loss = 0.70340, time: 23.59 sec\n",
      "epoch : 68,train loss = 0.73277, test loss = 0.83628, running test loss = 0.70533, time: 24.33 sec\n",
      "epoch : 69,train loss = 0.69698, test loss = 0.67948, running test loss = 0.70496, time: 24.33 sec\n",
      "epoch : 70,train loss = 0.68584, test loss = 0.64662, running test loss = 0.70414, time: 24.37 sec\n",
      "epoch : 71,train loss = 0.72805, test loss = 0.69344, running test loss = 0.70399, time: 24.35 sec\n",
      "epoch : 72,train loss = 0.72941, test loss = 0.77253, running test loss = 0.70493, time: 24.02 sec\n",
      "epoch : 73,train loss = 0.72459, test loss = 0.69240, running test loss = 0.70476, time: 23.73 sec\n",
      "epoch : 74,train loss = 0.66790, test loss = 0.80486, running test loss = 0.70609, time: 24.18 sec\n",
      "epoch : 75,train loss = 0.68336, test loss = 0.69171, running test loss = 0.70590, time: 24.57 sec\n",
      "epoch : 76,train loss = 0.66953, test loss = 0.62238, running test loss = 0.70482, time: 24.71 sec\n",
      "epoch : 77,train loss = 0.69076, test loss = 0.72175, running test loss = 0.70504, time: 24.20 sec\n",
      "epoch : 78,train loss = 0.68366, test loss = 0.72823, running test loss = 0.70533, time: 24.01 sec\n",
      "epoch : 79,train loss = 0.70367, test loss = 0.75380, running test loss = 0.70594, time: 24.36 sec\n",
      "epoch : 80,train loss = 0.72913, test loss = 0.74714, running test loss = 0.70644, time: 24.52 sec\n",
      "epoch : 81,train loss = 0.68877, test loss = 0.66253, running test loss = 0.70591, time: 24.13 sec\n",
      "epoch : 82,train loss = 0.71135, test loss = 0.64485, running test loss = 0.70517, time: 24.41 sec\n",
      "epoch : 83,train loss = 0.70647, test loss = 0.72748, running test loss = 0.70544, time: 24.45 sec\n",
      "epoch : 84,train loss = 0.67046, test loss = 0.75961, running test loss = 0.70608, time: 23.96 sec\n",
      "epoch : 85,train loss = 0.71530, test loss = 0.74855, running test loss = 0.70657, time: 24.36 sec\n",
      "epoch : 86,train loss = 0.71825, test loss = 0.62500, running test loss = 0.70563, time: 24.13 sec\n",
      "epoch : 87,train loss = 0.71919, test loss = 0.69315, running test loss = 0.70549, time: 23.25 sec\n",
      "epoch : 88,train loss = 0.74597, test loss = 0.74958, running test loss = 0.70599, time: 15.78 sec\n",
      "epoch : 89,train loss = 0.70139, test loss = 0.60668, running test loss = 0.70488, time: 15.47 sec\n",
      "epoch : 90,train loss = 0.69407, test loss = 0.71181, running test loss = 0.70496, time: 15.46 sec\n",
      "epoch : 91,train loss = 0.71000, test loss = 0.69915, running test loss = 0.70490, time: 15.47 sec\n",
      "epoch : 92,train loss = 0.70907, test loss = 0.61956, running test loss = 0.70398, time: 15.45 sec\n",
      "epoch : 93,train loss = 0.70072, test loss = 0.65671, running test loss = 0.70347, time: 15.46 sec\n",
      "epoch : 94,train loss = 0.70908, test loss = 0.64396, running test loss = 0.70285, time: 15.46 sec\n",
      "epoch : 95,train loss = 0.68302, test loss = 0.70103, running test loss = 0.70283, time: 15.47 sec\n",
      "epoch : 96,train loss = 0.70150, test loss = 0.60741, running test loss = 0.70185, time: 15.47 sec\n",
      "epoch : 97,train loss = 0.66782, test loss = 0.65455, running test loss = 0.70136, time: 15.47 sec\n",
      "epoch : 98,train loss = 0.69077, test loss = 0.68784, running test loss = 0.70123, time: 15.46 sec\n",
      "epoch : 99,train loss = 0.70464, test loss = 0.64366, running test loss = 0.70065, time: 15.47 sec\n",
      "epoch : 100,train loss = 0.66614, test loss = 0.73236, running test loss = 0.70096, time: 15.46 sec\n",
      "epoch : 101,train loss = 0.70632, test loss = 0.73895, running test loss = 0.70134, time: 15.45 sec\n",
      "epoch : 102,train loss = 0.68707, test loss = 0.72090, running test loss = 0.70153, time: 15.46 sec\n",
      "epoch : 103,train loss = 0.71793, test loss = 0.69263, running test loss = 0.70144, time: 15.46 sec\n",
      "epoch : 104,train loss = 0.73675, test loss = 0.67399, running test loss = 0.70118, time: 15.46 sec\n",
      "epoch : 105,train loss = 0.71460, test loss = 0.76538, running test loss = 0.70179, time: 15.46 sec\n",
      "epoch : 106,train loss = 0.72474, test loss = 0.70820, running test loss = 0.70185, time: 15.46 sec\n",
      "epoch : 107,train loss = 0.75193, test loss = 0.70992, running test loss = 0.70192, time: 15.47 sec\n",
      "epoch : 108,train loss = 0.67973, test loss = 0.68645, running test loss = 0.70178, time: 15.47 sec\n",
      "epoch : 109,train loss = 0.66329, test loss = 0.70281, running test loss = 0.70179, time: 15.46 sec\n",
      "epoch : 110,train loss = 0.67112, test loss = 0.65156, running test loss = 0.70134, time: 15.46 sec\n",
      "epoch : 111,train loss = 0.72296, test loss = 0.70503, running test loss = 0.70137, time: 15.47 sec\n",
      "epoch : 112,train loss = 0.71490, test loss = 0.76058, running test loss = 0.70189, time: 15.47 sec\n",
      "epoch : 113,train loss = 0.72404, test loss = 0.56325, running test loss = 0.70068, time: 15.46 sec\n",
      "epoch : 114,train loss = 0.66022, test loss = 0.68666, running test loss = 0.70055, time: 15.46 sec\n",
      "epoch : 115,train loss = 0.68855, test loss = 0.73106, running test loss = 0.70082, time: 15.48 sec\n",
      "epoch : 116,train loss = 0.73569, test loss = 0.74868, running test loss = 0.70123, time: 15.51 sec\n",
      "epoch : 117,train loss = 0.71011, test loss = 0.70248, running test loss = 0.70124, time: 15.50 sec\n",
      "epoch : 118,train loss = 0.70484, test loss = 0.71203, running test loss = 0.70133, time: 15.50 sec\n",
      "epoch : 119,train loss = 0.70647, test loss = 0.61175, running test loss = 0.70058, time: 15.50 sec\n",
      "epoch : 120,train loss = 0.68814, test loss = 0.70638, running test loss = 0.70063, time: 15.50 sec\n",
      "epoch : 121,train loss = 0.68949, test loss = 0.68365, running test loss = 0.70049, time: 15.50 sec\n",
      "epoch : 122,train loss = 0.70600, test loss = 0.74356, running test loss = 0.70084, time: 15.50 sec\n",
      "epoch : 123,train loss = 0.69876, test loss = 0.67459, running test loss = 0.70063, time: 15.49 sec\n",
      "epoch : 124,train loss = 0.67541, test loss = 0.79802, running test loss = 0.70141, time: 15.50 sec\n",
      "epoch : 125,train loss = 0.67848, test loss = 0.65369, running test loss = 0.70103, time: 15.48 sec\n",
      "epoch : 126,train loss = 0.67800, test loss = 0.79910, running test loss = 0.70180, time: 15.48 sec\n",
      "epoch : 127,train loss = 0.67212, test loss = 0.66419, running test loss = 0.70151, time: 15.48 sec\n",
      "epoch : 128,train loss = 0.69217, test loss = 0.78954, running test loss = 0.70219, time: 15.47 sec\n",
      "epoch : 129,train loss = 0.70463, test loss = 0.66324, running test loss = 0.70189, time: 15.48 sec\n",
      "epoch : 130,train loss = 0.68863, test loss = 0.68992, running test loss = 0.70180, time: 15.48 sec\n",
      "epoch : 131,train loss = 0.69924, test loss = 0.69888, running test loss = 0.70178, time: 15.47 sec\n",
      "epoch : 132,train loss = 0.69258, test loss = 0.64099, running test loss = 0.70132, time: 15.49 sec\n",
      "epoch : 133,train loss = 0.72376, test loss = 0.64237, running test loss = 0.70088, time: 15.48 sec\n",
      "epoch : 134,train loss = 0.70818, test loss = 0.70880, running test loss = 0.70094, time: 15.47 sec\n",
      "epoch : 135,train loss = 0.71106, test loss = 0.72509, running test loss = 0.70112, time: 15.48 sec\n",
      "epoch : 136,train loss = 0.68076, test loss = 0.70387, running test loss = 0.70114, time: 15.47 sec\n",
      "epoch : 137,train loss = 0.68930, test loss = 0.65904, running test loss = 0.70083, time: 15.47 sec\n",
      "epoch : 138,train loss = 0.65646, test loss = 0.70123, running test loss = 0.70083, time: 15.48 sec\n",
      "epoch : 139,train loss = 0.68786, test loss = 0.71354, running test loss = 0.70092, time: 15.47 sec\n",
      "epoch : 140,train loss = 0.69895, test loss = 0.62831, running test loss = 0.70041, time: 15.47 sec\n",
      "epoch : 141,train loss = 0.70773, test loss = 0.67249, running test loss = 0.70021, time: 15.47 sec\n",
      "epoch : 142,train loss = 0.68147, test loss = 0.69660, running test loss = 0.70019, time: 15.48 sec\n",
      "epoch : 143,train loss = 0.68579, test loss = 0.68047, running test loss = 0.70005, time: 15.48 sec\n",
      "epoch : 144,train loss = 0.67072, test loss = 0.64596, running test loss = 0.69968, time: 15.48 sec\n",
      "epoch : 145,train loss = 0.68627, test loss = 0.70015, running test loss = 0.69968, time: 15.48 sec\n",
      "epoch : 146,train loss = 0.69777, test loss = 0.65728, running test loss = 0.69939, time: 15.48 sec\n",
      "epoch : 147,train loss = 0.69573, test loss = 0.68620, running test loss = 0.69930, time: 15.49 sec\n",
      "epoch : 148,train loss = 0.68736, test loss = 0.64038, running test loss = 0.69891, time: 15.49 sec\n",
      "epoch : 149,train loss = 0.74877, test loss = 0.68475, running test loss = 0.69881, time: 15.49 sec\n",
      "epoch : 150,train loss = 0.72025, test loss = 0.78984, running test loss = 0.69942, time: 15.48 sec\n",
      "epoch : 151,train loss = 0.67747, test loss = 0.67264, running test loss = 0.69924, time: 15.49 sec\n",
      "epoch : 152,train loss = 0.69739, test loss = 0.77118, running test loss = 0.69971, time: 15.49 sec\n",
      "epoch : 153,train loss = 0.70058, test loss = 0.69574, running test loss = 0.69968, time: 15.49 sec\n",
      "epoch : 154,train loss = 0.67029, test loss = 0.79039, running test loss = 0.70027, time: 15.48 sec\n",
      "epoch : 155,train loss = 0.73699, test loss = 0.68195, running test loss = 0.70015, time: 15.49 sec\n",
      "epoch : 156,train loss = 0.68884, test loss = 0.76129, running test loss = 0.70054, time: 15.48 sec\n",
      "epoch : 157,train loss = 0.67185, test loss = 0.73720, running test loss = 0.70077, time: 15.48 sec\n",
      "epoch : 158,train loss = 0.68200, test loss = 0.77041, running test loss = 0.70121, time: 15.48 sec\n",
      "epoch : 159,train loss = 0.68287, test loss = 0.63853, running test loss = 0.70082, time: 15.49 sec\n",
      "epoch : 160,train loss = 0.69617, test loss = 0.64041, running test loss = 0.70044, time: 15.48 sec\n",
      "epoch : 161,train loss = 0.75298, test loss = 0.62727, running test loss = 0.69999, time: 15.49 sec\n",
      "epoch : 162,train loss = 0.65692, test loss = 0.68742, running test loss = 0.69992, time: 15.48 sec\n",
      "epoch : 163,train loss = 0.70952, test loss = 0.70914, running test loss = 0.69997, time: 15.48 sec\n",
      "epoch : 164,train loss = 0.65774, test loss = 0.73928, running test loss = 0.70021, time: 15.48 sec\n",
      "epoch : 165,train loss = 0.70071, test loss = 0.79733, running test loss = 0.70080, time: 15.48 sec\n",
      "epoch : 166,train loss = 0.70635, test loss = 0.68157, running test loss = 0.70068, time: 15.48 sec\n",
      "epoch : 167,train loss = 0.67425, test loss = 0.76704, running test loss = 0.70108, time: 15.49 sec\n",
      "epoch : 168,train loss = 0.70714, test loss = 0.75299, running test loss = 0.70138, time: 15.48 sec\n",
      "epoch : 169,train loss = 0.64057, test loss = 0.65579, running test loss = 0.70111, time: 15.48 sec\n",
      "epoch : 170,train loss = 0.66020, test loss = 0.65337, running test loss = 0.70084, time: 15.48 sec\n",
      "epoch : 171,train loss = 0.69127, test loss = 0.67584, running test loss = 0.70069, time: 15.49 sec\n",
      "epoch : 172,train loss = 0.69928, test loss = 0.64964, running test loss = 0.70039, time: 15.49 sec\n",
      "epoch : 173,train loss = 0.72375, test loss = 0.65589, running test loss = 0.70014, time: 15.49 sec\n",
      "epoch : 174,train loss = 0.63560, test loss = 0.69273, running test loss = 0.70010, time: 15.49 sec\n",
      "epoch : 175,train loss = 0.69844, test loss = 0.65202, running test loss = 0.69982, time: 15.49 sec\n",
      "epoch : 176,train loss = 0.71358, test loss = 0.61520, running test loss = 0.69935, time: 15.49 sec\n",
      "epoch : 177,train loss = 0.69785, test loss = 0.78425, running test loss = 0.69982, time: 15.49 sec\n",
      "epoch : 178,train loss = 0.69488, test loss = 0.70261, running test loss = 0.69984, time: 15.49 sec\n",
      "epoch : 179,train loss = 0.69604, test loss = 0.64455, running test loss = 0.69953, time: 15.47 sec\n",
      "epoch : 180,train loss = 0.70085, test loss = 0.61546, running test loss = 0.69907, time: 15.49 sec\n",
      "epoch : 181,train loss = 0.69399, test loss = 0.64387, running test loss = 0.69876, time: 15.48 sec\n",
      "epoch : 182,train loss = 0.70402, test loss = 0.64734, running test loss = 0.69848, time: 15.49 sec\n",
      "epoch : 183,train loss = 0.68267, test loss = 0.63293, running test loss = 0.69813, time: 15.48 sec\n",
      "epoch : 184,train loss = 0.67796, test loss = 0.70197, running test loss = 0.69815, time: 15.48 sec\n",
      "epoch : 185,train loss = 0.74531, test loss = 0.73680, running test loss = 0.69835, time: 15.49 sec\n",
      "epoch : 186,train loss = 0.70733, test loss = 0.66767, running test loss = 0.69819, time: 15.49 sec\n",
      "epoch : 187,train loss = 0.67430, test loss = 0.64328, running test loss = 0.69790, time: 15.49 sec\n",
      "epoch : 188,train loss = 0.71086, test loss = 0.61587, running test loss = 0.69746, time: 15.49 sec\n",
      "epoch : 189,train loss = 0.74163, test loss = 0.63523, running test loss = 0.69714, time: 15.49 sec\n",
      "epoch : 190,train loss = 0.66830, test loss = 0.73771, running test loss = 0.69735, time: 15.49 sec\n",
      "epoch : 191,train loss = 0.69297, test loss = 0.71032, running test loss = 0.69742, time: 15.49 sec\n",
      "epoch : 192,train loss = 0.71187, test loss = 0.64608, running test loss = 0.69715, time: 15.49 sec\n",
      "epoch : 193,train loss = 0.69339, test loss = 0.74335, running test loss = 0.69739, time: 15.48 sec\n",
      "epoch : 194,train loss = 0.70964, test loss = 0.75861, running test loss = 0.69770, time: 15.49 sec\n",
      "epoch : 195,train loss = 0.73686, test loss = 0.68240, running test loss = 0.69762, time: 15.48 sec\n",
      "epoch : 196,train loss = 0.68408, test loss = 0.65549, running test loss = 0.69741, time: 15.49 sec\n",
      "epoch : 197,train loss = 0.69857, test loss = 0.63082, running test loss = 0.69707, time: 15.49 sec\n",
      "epoch : 198,train loss = 0.71502, test loss = 0.64217, running test loss = 0.69680, time: 15.49 sec\n",
      "epoch : 199,train loss = 0.67792, test loss = 0.66763, running test loss = 0.69665, time: 15.48 sec\n",
      "epoch : 200,train loss = 0.73479, test loss = 0.77693, running test loss = 0.69705, time: 15.48 sec\n",
      "epoch : 201,train loss = 0.69089, test loss = 0.68143, running test loss = 0.69697, time: 15.49 sec\n",
      "epoch : 202,train loss = 0.69447, test loss = 0.69580, running test loss = 0.69697, time: 15.48 sec\n",
      "epoch : 203,train loss = 0.71295, test loss = 0.68191, running test loss = 0.69690, time: 15.49 sec\n",
      "epoch : 204,train loss = 0.71008, test loss = 0.74238, running test loss = 0.69712, time: 15.50 sec\n",
      "epoch : 205,train loss = 0.70683, test loss = 0.66352, running test loss = 0.69695, time: 15.49 sec\n",
      "epoch : 206,train loss = 0.68080, test loss = 0.80288, running test loss = 0.69747, time: 15.48 sec\n",
      "epoch : 207,train loss = 0.68673, test loss = 0.79511, running test loss = 0.69794, time: 15.48 sec\n",
      "epoch : 208,train loss = 0.68060, test loss = 0.68489, running test loss = 0.69787, time: 15.49 sec\n",
      "epoch : 209,train loss = 0.72114, test loss = 0.68649, running test loss = 0.69782, time: 15.50 sec\n",
      "epoch : 210,train loss = 0.70528, test loss = 0.65939, running test loss = 0.69764, time: 15.48 sec\n",
      "epoch : 211,train loss = 0.71829, test loss = 0.67022, running test loss = 0.69751, time: 15.49 sec\n",
      "epoch : 212,train loss = 0.72845, test loss = 0.61979, running test loss = 0.69714, time: 15.49 sec\n",
      "epoch : 213,train loss = 0.68329, test loss = 0.73274, running test loss = 0.69731, time: 15.48 sec\n",
      "epoch : 214,train loss = 0.68673, test loss = 0.66778, running test loss = 0.69717, time: 15.48 sec\n",
      "epoch : 215,train loss = 0.68120, test loss = 0.67440, running test loss = 0.69707, time: 15.48 sec\n",
      "epoch : 216,train loss = 0.70520, test loss = 0.59977, running test loss = 0.69662, time: 15.49 sec\n",
      "epoch : 217,train loss = 0.65224, test loss = 0.69520, running test loss = 0.69661, time: 15.50 sec\n",
      "epoch : 218,train loss = 0.68029, test loss = 0.71717, running test loss = 0.69670, time: 15.50 sec\n",
      "epoch : 219,train loss = 0.69133, test loss = 0.64034, running test loss = 0.69645, time: 15.50 sec\n",
      "epoch : 220,train loss = 0.69953, test loss = 0.74049, running test loss = 0.69665, time: 15.49 sec\n",
      "epoch : 221,train loss = 0.68258, test loss = 0.76179, running test loss = 0.69694, time: 15.49 sec\n",
      "epoch : 222,train loss = 0.69207, test loss = 0.75875, running test loss = 0.69722, time: 15.48 sec\n",
      "epoch : 223,train loss = 0.73430, test loss = 0.72213, running test loss = 0.69733, time: 15.49 sec\n",
      "epoch : 224,train loss = 0.69018, test loss = 0.67557, running test loss = 0.69723, time: 15.49 sec\n",
      "epoch : 225,train loss = 0.68435, test loss = 0.90187, running test loss = 0.69814, time: 15.48 sec\n",
      "epoch : 226,train loss = 0.68487, test loss = 0.73585, running test loss = 0.69830, time: 15.49 sec\n",
      "epoch : 227,train loss = 0.68621, test loss = 0.74597, running test loss = 0.69851, time: 15.49 sec\n",
      "epoch : 228,train loss = 0.69339, test loss = 0.73522, running test loss = 0.69867, time: 15.49 sec\n",
      "epoch : 229,train loss = 0.73632, test loss = 0.70322, running test loss = 0.69869, time: 15.48 sec\n",
      "epoch : 230,train loss = 0.70946, test loss = 0.63907, running test loss = 0.69844, time: 15.48 sec\n",
      "epoch : 231,train loss = 0.72731, test loss = 0.77511, running test loss = 0.69877, time: 15.48 sec\n",
      "epoch : 232,train loss = 0.72232, test loss = 0.71530, running test loss = 0.69884, time: 15.48 sec\n",
      "epoch : 233,train loss = 0.69501, test loss = 0.74119, running test loss = 0.69902, time: 15.49 sec\n",
      "epoch : 234,train loss = 0.72143, test loss = 0.69686, running test loss = 0.69901, time: 15.39 sec\n",
      "epoch : 235,train loss = 0.68215, test loss = 0.68358, running test loss = 0.69894, time: 15.38 sec\n",
      "epoch : 236,train loss = 0.65205, test loss = 0.69894, running test loss = 0.69894, time: 15.37 sec\n",
      "epoch : 237,train loss = 0.69665, test loss = 0.74341, running test loss = 0.69913, time: 15.38 sec\n",
      "epoch : 238,train loss = 0.65407, test loss = 0.71263, running test loss = 0.69919, time: 15.38 sec\n",
      "epoch : 239,train loss = 0.69024, test loss = 0.75621, running test loss = 0.69942, time: 15.36 sec\n",
      "epoch : 240,train loss = 0.71272, test loss = 0.69807, running test loss = 0.69942, time: 15.40 sec\n",
      "epoch : 241,train loss = 0.71568, test loss = 0.80463, running test loss = 0.69985, time: 15.46 sec\n",
      "epoch : 242,train loss = 0.71205, test loss = 0.72674, running test loss = 0.69996, time: 15.46 sec\n",
      "epoch : 243,train loss = 0.68851, test loss = 0.63970, running test loss = 0.69972, time: 15.44 sec\n",
      "epoch : 244,train loss = 0.65361, test loss = 0.75720, running test loss = 0.69995, time: 15.46 sec\n",
      "epoch : 245,train loss = 0.69802, test loss = 0.68335, running test loss = 0.69988, time: 15.45 sec\n",
      "epoch : 246,train loss = 0.71711, test loss = 0.81589, running test loss = 0.70035, time: 15.45 sec\n",
      "epoch : 247,train loss = 0.65485, test loss = 0.63383, running test loss = 0.70009, time: 15.46 sec\n",
      "epoch : 248,train loss = 0.67937, test loss = 0.79367, running test loss = 0.70046, time: 15.46 sec\n",
      "epoch : 249,train loss = 0.70061, test loss = 0.64502, running test loss = 0.70024, time: 15.46 sec\n",
      "epoch : 250,train loss = 0.66059, test loss = 0.76711, running test loss = 0.70051, time: 15.46 sec\n",
      "epoch : 251,train loss = 0.69571, test loss = 0.66676, running test loss = 0.70037, time: 15.46 sec\n",
      "epoch : 252,train loss = 0.72001, test loss = 0.79920, running test loss = 0.70076, time: 15.46 sec\n",
      "epoch : 253,train loss = 0.70486, test loss = 0.76038, running test loss = 0.70100, time: 15.45 sec\n",
      "epoch : 254,train loss = 0.67764, test loss = 0.70393, running test loss = 0.70101, time: 15.45 sec\n",
      "epoch : 255,train loss = 0.71321, test loss = 0.75751, running test loss = 0.70123, time: 15.46 sec\n",
      "epoch : 256,train loss = 0.70227, test loss = 0.73082, running test loss = 0.70135, time: 15.45 sec\n",
      "epoch : 257,train loss = 0.71065, test loss = 0.67363, running test loss = 0.70124, time: 15.45 sec\n",
      "epoch : 258,train loss = 0.73057, test loss = 0.75197, running test loss = 0.70143, time: 15.45 sec\n",
      "epoch : 259,train loss = 0.67438, test loss = 0.72477, running test loss = 0.70152, time: 15.45 sec\n",
      "epoch : 260,train loss = 0.67808, test loss = 0.68462, running test loss = 0.70146, time: 15.45 sec\n",
      "epoch : 261,train loss = 0.67769, test loss = 0.63741, running test loss = 0.70121, time: 15.45 sec\n",
      "epoch : 262,train loss = 0.70723, test loss = 0.72744, running test loss = 0.70131, time: 15.45 sec\n",
      "epoch : 263,train loss = 0.69084, test loss = 0.64167, running test loss = 0.70109, time: 15.45 sec\n",
      "epoch : 264,train loss = 0.71283, test loss = 0.69823, running test loss = 0.70108, time: 15.44 sec\n",
      "epoch : 265,train loss = 0.71961, test loss = 0.69503, running test loss = 0.70105, time: 15.45 sec\n",
      "epoch : 266,train loss = 0.70543, test loss = 0.68301, running test loss = 0.70099, time: 15.44 sec\n",
      "epoch : 267,train loss = 0.68352, test loss = 0.76111, running test loss = 0.70121, time: 15.46 sec\n",
      "epoch : 268,train loss = 0.73125, test loss = 0.67866, running test loss = 0.70113, time: 15.45 sec\n",
      "epoch : 269,train loss = 0.67485, test loss = 0.63565, running test loss = 0.70088, time: 15.45 sec\n",
      "epoch : 270,train loss = 0.70554, test loss = 0.73046, running test loss = 0.70099, time: 15.46 sec\n",
      "epoch : 271,train loss = 0.67632, test loss = 0.70657, running test loss = 0.70101, time: 15.45 sec\n",
      "epoch : 272,train loss = 0.71158, test loss = 0.74473, running test loss = 0.70117, time: 15.44 sec\n",
      "epoch : 273,train loss = 0.71099, test loss = 0.71208, running test loss = 0.70121, time: 15.44 sec\n",
      "epoch : 274,train loss = 0.69415, test loss = 0.76006, running test loss = 0.70143, time: 15.46 sec\n",
      "epoch : 275,train loss = 0.72032, test loss = 0.68982, running test loss = 0.70139, time: 15.45 sec\n",
      "epoch : 276,train loss = 0.69033, test loss = 0.72013, running test loss = 0.70145, time: 15.45 sec\n",
      "epoch : 277,train loss = 0.67833, test loss = 0.65250, running test loss = 0.70128, time: 15.46 sec\n",
      "epoch : 278,train loss = 0.68728, test loss = 0.73836, running test loss = 0.70141, time: 15.45 sec\n",
      "epoch : 279,train loss = 0.72011, test loss = 0.79984, running test loss = 0.70176, time: 15.45 sec\n",
      "epoch : 280,train loss = 0.70027, test loss = 0.72085, running test loss = 0.70183, time: 15.45 sec\n",
      "epoch : 281,train loss = 0.68806, test loss = 0.65878, running test loss = 0.70168, time: 15.46 sec\n",
      "epoch : 282,train loss = 0.68250, test loss = 0.76709, running test loss = 0.70191, time: 15.71 sec\n",
      "epoch : 283,train loss = 0.69532, test loss = 0.72346, running test loss = 0.70198, time: 15.78 sec\n",
      "epoch : 284,train loss = 0.73207, test loss = 0.74144, running test loss = 0.70212, time: 15.87 sec\n",
      "epoch : 285,train loss = 0.68811, test loss = 0.72364, running test loss = 0.70220, time: 15.78 sec\n",
      "epoch : 286,train loss = 0.69470, test loss = 0.69832, running test loss = 0.70218, time: 15.75 sec\n",
      "epoch : 287,train loss = 0.72609, test loss = 0.68504, running test loss = 0.70213, time: 15.76 sec\n",
      "epoch : 288,train loss = 0.67236, test loss = 0.75770, running test loss = 0.70232, time: 15.82 sec\n",
      "epoch : 289,train loss = 0.66062, test loss = 0.62073, running test loss = 0.70204, time: 15.82 sec\n",
      "epoch : 290,train loss = 0.70886, test loss = 0.73467, running test loss = 0.70215, time: 15.83 sec\n",
      "epoch : 291,train loss = 0.70138, test loss = 0.61371, running test loss = 0.70185, time: 15.82 sec\n",
      "epoch : 292,train loss = 0.71455, test loss = 0.72889, running test loss = 0.70194, time: 15.83 sec\n",
      "epoch : 293,train loss = 0.70154, test loss = 0.71639, running test loss = 0.70199, time: 15.82 sec\n",
      "epoch : 294,train loss = 0.69294, test loss = 0.67080, running test loss = 0.70188, time: 15.84 sec\n",
      "epoch : 295,train loss = 0.70254, test loss = 0.78656, running test loss = 0.70217, time: 15.81 sec\n",
      "epoch : 296,train loss = 0.68279, test loss = 0.74906, running test loss = 0.70233, time: 18.01 sec\n",
      "epoch : 297,train loss = 0.66717, test loss = 0.73887, running test loss = 0.70245, time: 28.40 sec\n",
      "epoch : 298,train loss = 0.66426, test loss = 0.63175, running test loss = 0.70221, time: 27.02 sec\n",
      "epoch : 299,train loss = 0.70346, test loss = 0.80349, running test loss = 0.70255, time: 15.70 sec\n",
      "epoch : 300,train loss = 0.72223, test loss = 0.70663, running test loss = 0.70256, time: 15.73 sec\n",
      "epoch : 301,train loss = 0.67159, test loss = 0.62591, running test loss = 0.70231, time: 15.82 sec\n",
      "epoch : 302,train loss = 0.72068, test loss = 0.66059, running test loss = 0.70217, time: 15.82 sec\n",
      "epoch : 303,train loss = 0.71153, test loss = 0.69072, running test loss = 0.70213, time: 15.81 sec\n",
      "epoch : 304,train loss = 0.73615, test loss = 0.70463, running test loss = 0.70214, time: 15.83 sec\n",
      "epoch : 305,train loss = 0.70455, test loss = 0.74587, running test loss = 0.70228, time: 17.14 sec\n",
      "epoch : 306,train loss = 0.71858, test loss = 0.72379, running test loss = 0.70235, time: 21.86 sec\n",
      "epoch : 307,train loss = 0.73035, test loss = 0.73280, running test loss = 0.70245, time: 16.17 sec\n",
      "epoch : 308,train loss = 0.69938, test loss = 0.70396, running test loss = 0.70246, time: 17.50 sec\n",
      "epoch : 309,train loss = 0.69846, test loss = 0.68681, running test loss = 0.70241, time: 17.52 sec\n",
      "epoch : 310,train loss = 0.68266, test loss = 0.73753, running test loss = 0.70252, time: 17.54 sec\n",
      "epoch : 311,train loss = 0.68260, test loss = 0.74863, running test loss = 0.70267, time: 17.51 sec\n",
      "epoch : 312,train loss = 0.66215, test loss = 0.70089, running test loss = 0.70266, time: 17.51 sec\n",
      "epoch : 313,train loss = 0.71525, test loss = 0.79041, running test loss = 0.70294, time: 17.51 sec\n",
      "epoch : 314,train loss = 0.69207, test loss = 0.67152, running test loss = 0.70284, time: 17.48 sec\n",
      "epoch : 315,train loss = 0.69639, test loss = 0.69729, running test loss = 0.70283, time: 17.44 sec\n",
      "epoch : 316,train loss = 0.66384, test loss = 0.82349, running test loss = 0.70321, time: 17.41 sec\n",
      "epoch : 317,train loss = 0.71674, test loss = 0.75045, running test loss = 0.70335, time: 17.52 sec\n",
      "epoch : 318,train loss = 0.72641, test loss = 0.62799, running test loss = 0.70312, time: 17.50 sec\n",
      "epoch : 319,train loss = 0.72190, test loss = 0.80697, running test loss = 0.70344, time: 17.49 sec\n",
      "epoch : 320,train loss = 0.68579, test loss = 0.72755, running test loss = 0.70352, time: 17.49 sec\n",
      "epoch : 321,train loss = 0.67890, test loss = 0.79962, running test loss = 0.70382, time: 17.54 sec\n",
      "epoch : 322,train loss = 0.69741, test loss = 0.84155, running test loss = 0.70424, time: 17.49 sec\n",
      "epoch : 323,train loss = 0.64935, test loss = 0.66062, running test loss = 0.70411, time: 17.44 sec\n",
      "epoch : 324,train loss = 0.70046, test loss = 0.79677, running test loss = 0.70439, time: 17.47 sec\n",
      "epoch : 325,train loss = 0.71039, test loss = 0.67851, running test loss = 0.70431, time: 17.57 sec\n",
      "epoch : 326,train loss = 0.71004, test loss = 0.71606, running test loss = 0.70435, time: 17.49 sec\n",
      "epoch : 327,train loss = 0.69514, test loss = 0.69599, running test loss = 0.70432, time: 17.46 sec\n",
      "epoch : 328,train loss = 0.68057, test loss = 0.66958, running test loss = 0.70422, time: 17.54 sec\n",
      "epoch : 329,train loss = 0.69578, test loss = 0.67464, running test loss = 0.70413, time: 17.46 sec\n",
      "epoch : 330,train loss = 0.70023, test loss = 0.68811, running test loss = 0.70408, time: 17.51 sec\n",
      "epoch : 331,train loss = 0.68111, test loss = 0.75213, running test loss = 0.70423, time: 17.49 sec\n",
      "epoch : 332,train loss = 0.70838, test loss = 0.66764, running test loss = 0.70412, time: 17.46 sec\n",
      "epoch : 333,train loss = 0.70322, test loss = 0.66817, running test loss = 0.70401, time: 17.49 sec\n",
      "epoch : 334,train loss = 0.69409, test loss = 0.65774, running test loss = 0.70387, time: 17.54 sec\n",
      "epoch : 335,train loss = 0.72646, test loss = 0.68626, running test loss = 0.70382, time: 17.46 sec\n",
      "epoch : 336,train loss = 0.69748, test loss = 0.75401, running test loss = 0.70397, time: 17.40 sec\n",
      "epoch : 337,train loss = 0.69009, test loss = 0.68965, running test loss = 0.70392, time: 17.55 sec\n",
      "epoch : 338,train loss = 0.64139, test loss = 0.76754, running test loss = 0.70411, time: 17.47 sec\n",
      "epoch : 339,train loss = 0.66663, test loss = 0.76209, running test loss = 0.70428, time: 17.54 sec\n",
      "epoch : 340,train loss = 0.69080, test loss = 0.70288, running test loss = 0.70428, time: 17.47 sec\n",
      "epoch : 341,train loss = 0.70944, test loss = 0.64890, running test loss = 0.70412, time: 17.44 sec\n",
      "epoch : 342,train loss = 0.67962, test loss = 0.85963, running test loss = 0.70457, time: 17.42 sec\n",
      "epoch : 343,train loss = 0.63824, test loss = 0.71574, running test loss = 0.70460, time: 17.44 sec\n",
      "epoch : 344,train loss = 0.70805, test loss = 0.60744, running test loss = 0.70432, time: 17.49 sec\n",
      "epoch : 345,train loss = 0.65644, test loss = 0.66994, running test loss = 0.70422, time: 17.46 sec\n",
      "epoch : 346,train loss = 0.71451, test loss = 0.64068, running test loss = 0.70404, time: 17.50 sec\n",
      "epoch : 347,train loss = 0.65585, test loss = 0.71340, running test loss = 0.70406, time: 17.50 sec\n",
      "epoch : 348,train loss = 0.68680, test loss = 0.68416, running test loss = 0.70401, time: 17.50 sec\n",
      "epoch : 349,train loss = 0.68311, test loss = 0.69595, running test loss = 0.70398, time: 17.53 sec\n",
      "epoch : 350,train loss = 0.69528, test loss = 0.70273, running test loss = 0.70398, time: 17.49 sec\n",
      "epoch : 351,train loss = 0.73287, test loss = 0.77325, running test loss = 0.70418, time: 17.45 sec\n",
      "epoch : 352,train loss = 0.72321, test loss = 0.71594, running test loss = 0.70421, time: 17.58 sec\n",
      "epoch : 353,train loss = 0.70201, test loss = 0.65635, running test loss = 0.70408, time: 17.44 sec\n",
      "epoch : 354,train loss = 0.71282, test loss = 0.69004, running test loss = 0.70404, time: 17.50 sec\n",
      "epoch : 355,train loss = 0.69414, test loss = 0.75496, running test loss = 0.70418, time: 17.43 sec\n",
      "epoch : 356,train loss = 0.71956, test loss = 0.77332, running test loss = 0.70437, time: 17.53 sec\n",
      "epoch : 357,train loss = 0.67904, test loss = 0.80536, running test loss = 0.70466, time: 17.47 sec\n",
      "epoch : 358,train loss = 0.70820, test loss = 0.71684, running test loss = 0.70469, time: 17.51 sec\n",
      "epoch : 359,train loss = 0.68441, test loss = 0.75212, running test loss = 0.70482, time: 17.47 sec\n",
      "epoch : 360,train loss = 0.69310, test loss = 0.69699, running test loss = 0.70480, time: 17.42 sec\n",
      "epoch : 361,train loss = 0.69859, test loss = 0.72789, running test loss = 0.70486, time: 17.47 sec\n",
      "epoch : 362,train loss = 0.67845, test loss = 0.73215, running test loss = 0.70494, time: 17.46 sec\n",
      "epoch : 363,train loss = 0.66760, test loss = 0.69861, running test loss = 0.70492, time: 17.56 sec\n",
      "epoch : 364,train loss = 0.68055, test loss = 0.68834, running test loss = 0.70488, time: 17.48 sec\n",
      "epoch : 365,train loss = 0.71991, test loss = 0.60795, running test loss = 0.70461, time: 17.46 sec\n",
      "epoch : 366,train loss = 0.66315, test loss = 0.56425, running test loss = 0.70423, time: 17.42 sec\n",
      "epoch : 367,train loss = 0.66905, test loss = 0.65617, running test loss = 0.70410, time: 17.47 sec\n",
      "epoch : 368,train loss = 0.69377, test loss = 0.65341, running test loss = 0.70396, time: 17.55 sec\n",
      "epoch : 369,train loss = 0.70315, test loss = 0.80395, running test loss = 0.70423, time: 17.44 sec\n",
      "epoch : 370,train loss = 0.69169, test loss = 0.71753, running test loss = 0.70427, time: 17.49 sec\n",
      "epoch : 371,train loss = 0.69972, test loss = 0.66677, running test loss = 0.70417, time: 17.49 sec\n",
      "epoch : 372,train loss = 0.66414, test loss = 0.68765, running test loss = 0.70412, time: 17.50 sec\n",
      "epoch : 373,train loss = 0.69652, test loss = 0.73986, running test loss = 0.70422, time: 17.46 sec\n",
      "epoch : 374,train loss = 0.70782, test loss = 0.59431, running test loss = 0.70392, time: 17.56 sec\n",
      "epoch : 375,train loss = 0.71818, test loss = 0.59538, running test loss = 0.70363, time: 17.42 sec\n",
      "epoch : 376,train loss = 0.74351, test loss = 0.84302, running test loss = 0.70400, time: 17.47 sec\n",
      "epoch : 377,train loss = 0.71616, test loss = 0.80155, running test loss = 0.70426, time: 17.47 sec\n",
      "epoch : 378,train loss = 0.70494, test loss = 0.65352, running test loss = 0.70413, time: 17.51 sec\n",
      "epoch : 379,train loss = 0.66804, test loss = 0.80695, running test loss = 0.70440, time: 17.53 sec\n",
      "epoch : 380,train loss = 0.68239, test loss = 0.75725, running test loss = 0.70454, time: 17.41 sec\n",
      "epoch : 381,train loss = 0.68838, test loss = 0.74276, running test loss = 0.70464, time: 17.50 sec\n",
      "epoch : 382,train loss = 0.66089, test loss = 0.73152, running test loss = 0.70471, time: 17.49 sec\n",
      "epoch : 383,train loss = 0.66660, test loss = 0.71193, running test loss = 0.70473, time: 17.44 sec\n",
      "epoch : 384,train loss = 0.69266, test loss = 0.77225, running test loss = 0.70490, time: 17.52 sec\n",
      "epoch : 385,train loss = 0.70805, test loss = 0.71283, running test loss = 0.70492, time: 17.49 sec\n",
      "epoch : 386,train loss = 0.73188, test loss = 0.66447, running test loss = 0.70482, time: 17.49 sec\n",
      "epoch : 387,train loss = 0.67646, test loss = 0.70948, running test loss = 0.70483, time: 17.50 sec\n",
      "epoch : 388,train loss = 0.69648, test loss = 0.75130, running test loss = 0.70495, time: 17.49 sec\n",
      "epoch : 389,train loss = 0.68953, test loss = 0.76635, running test loss = 0.70511, time: 17.43 sec\n",
      "epoch : 390,train loss = 0.70692, test loss = 0.72412, running test loss = 0.70516, time: 17.51 sec\n",
      "epoch : 391,train loss = 0.70054, test loss = 0.65699, running test loss = 0.70503, time: 17.52 sec\n",
      "epoch : 392,train loss = 0.70629, test loss = 0.64282, running test loss = 0.70487, time: 17.45 sec\n",
      "epoch : 393,train loss = 0.69170, test loss = 0.78953, running test loss = 0.70509, time: 17.52 sec\n",
      "epoch : 394,train loss = 0.68279, test loss = 0.63079, running test loss = 0.70490, time: 17.42 sec\n",
      "epoch : 395,train loss = 0.70939, test loss = 0.64353, running test loss = 0.70475, time: 17.39 sec\n",
      "epoch : 396,train loss = 0.71420, test loss = 0.60218, running test loss = 0.70449, time: 17.46 sec\n",
      "epoch : 397,train loss = 0.69453, test loss = 0.75765, running test loss = 0.70462, time: 17.47 sec\n",
      "epoch : 398,train loss = 0.71111, test loss = 0.63582, running test loss = 0.70445, time: 17.46 sec\n",
      "epoch : 399,train loss = 0.73847, test loss = 0.68665, running test loss = 0.70440, time: 17.55 sec\n",
      "epoch : 400,train loss = 0.68493, test loss = 0.64693, running test loss = 0.70426, time: 17.46 sec\n",
      "epoch : 401,train loss = 0.70494, test loss = 0.76610, running test loss = 0.70442, time: 17.47 sec\n",
      "epoch : 402,train loss = 0.72941, test loss = 0.71938, running test loss = 0.70445, time: 17.52 sec\n",
      "epoch : 403,train loss = 0.69218, test loss = 0.73712, running test loss = 0.70453, time: 17.49 sec\n",
      "epoch : 404,train loss = 0.68573, test loss = 0.67694, running test loss = 0.70447, time: 17.48 sec\n",
      "epoch : 405,train loss = 0.68923, test loss = 0.64483, running test loss = 0.70432, time: 17.51 sec\n",
      "epoch : 406,train loss = 0.69658, test loss = 0.70241, running test loss = 0.70431, time: 17.47 sec\n",
      "epoch : 407,train loss = 0.68785, test loss = 0.75021, running test loss = 0.70443, time: 17.47 sec\n",
      "epoch : 408,train loss = 0.72623, test loss = 0.72078, running test loss = 0.70447, time: 17.47 sec\n",
      "epoch : 409,train loss = 0.71124, test loss = 0.71468, running test loss = 0.70449, time: 17.50 sec\n",
      "epoch : 410,train loss = 0.70449, test loss = 0.76212, running test loss = 0.70463, time: 17.48 sec\n",
      "epoch : 411,train loss = 0.74401, test loss = 0.71128, running test loss = 0.70465, time: 17.42 sec\n",
      "epoch : 412,train loss = 0.73744, test loss = 0.76358, running test loss = 0.70479, time: 17.50 sec\n",
      "epoch : 413,train loss = 0.71396, test loss = 0.80490, running test loss = 0.70503, time: 17.54 sec\n",
      "epoch : 414,train loss = 0.66781, test loss = 0.66187, running test loss = 0.70493, time: 17.51 sec\n",
      "epoch : 415,train loss = 0.68493, test loss = 0.78578, running test loss = 0.70512, time: 17.50 sec\n",
      "epoch : 416,train loss = 0.72217, test loss = 0.66505, running test loss = 0.70503, time: 17.53 sec\n",
      "epoch : 417,train loss = 0.70649, test loss = 0.65701, running test loss = 0.70491, time: 17.51 sec\n",
      "epoch : 418,train loss = 0.70202, test loss = 0.74489, running test loss = 0.70501, time: 17.45 sec\n",
      "epoch : 419,train loss = 0.67452, test loss = 0.71851, running test loss = 0.70504, time: 17.49 sec\n",
      "epoch : 420,train loss = 0.70263, test loss = 0.73218, running test loss = 0.70510, time: 17.49 sec\n",
      "epoch : 421,train loss = 0.73071, test loss = 0.72274, running test loss = 0.70515, time: 17.47 sec\n",
      "epoch : 422,train loss = 0.69049, test loss = 0.66205, running test loss = 0.70504, time: 17.47 sec\n",
      "epoch : 423,train loss = 0.67641, test loss = 0.72052, running test loss = 0.70508, time: 17.50 sec\n",
      "epoch : 424,train loss = 0.68619, test loss = 0.70618, running test loss = 0.70508, time: 17.45 sec\n",
      "epoch : 425,train loss = 0.67924, test loss = 0.72137, running test loss = 0.70512, time: 17.46 sec\n",
      "epoch : 426,train loss = 0.71760, test loss = 0.67255, running test loss = 0.70504, time: 17.50 sec\n",
      "epoch : 427,train loss = 0.69549, test loss = 0.70046, running test loss = 0.70503, time: 17.47 sec\n",
      "epoch : 428,train loss = 0.67570, test loss = 0.74103, running test loss = 0.70512, time: 17.44 sec\n",
      "epoch : 429,train loss = 0.69971, test loss = 0.65909, running test loss = 0.70501, time: 17.45 sec\n",
      "epoch : 430,train loss = 0.67578, test loss = 0.72205, running test loss = 0.70505, time: 17.53 sec\n",
      "epoch : 431,train loss = 0.70009, test loss = 0.66999, running test loss = 0.70497, time: 17.42 sec\n",
      "epoch : 432,train loss = 0.73593, test loss = 0.75830, running test loss = 0.70509, time: 17.45 sec\n",
      "epoch : 433,train loss = 0.72424, test loss = 0.75466, running test loss = 0.70521, time: 17.48 sec\n",
      "epoch : 434,train loss = 0.67175, test loss = 0.77361, running test loss = 0.70536, time: 17.51 sec\n",
      "epoch : 435,train loss = 0.71920, test loss = 0.68485, running test loss = 0.70532, time: 17.48 sec\n",
      "epoch : 436,train loss = 0.68289, test loss = 0.63571, running test loss = 0.70516, time: 17.56 sec\n",
      "epoch : 437,train loss = 0.68551, test loss = 0.83065, running test loss = 0.70544, time: 17.55 sec\n",
      "epoch : 438,train loss = 0.68918, test loss = 0.75888, running test loss = 0.70557, time: 17.47 sec\n",
      "epoch : 439,train loss = 0.68862, test loss = 0.62998, running test loss = 0.70539, time: 17.42 sec\n",
      "epoch : 440,train loss = 0.65008, test loss = 0.70250, running test loss = 0.70539, time: 17.54 sec\n",
      "epoch : 441,train loss = 0.67977, test loss = 0.65706, running test loss = 0.70528, time: 17.43 sec\n",
      "epoch : 442,train loss = 0.69529, test loss = 0.75726, running test loss = 0.70540, time: 17.45 sec\n",
      "epoch : 443,train loss = 0.68352, test loss = 0.71038, running test loss = 0.70541, time: 17.48 sec\n",
      "epoch : 444,train loss = 0.69943, test loss = 0.79631, running test loss = 0.70561, time: 17.54 sec\n",
      "epoch : 445,train loss = 0.68295, test loss = 0.65720, running test loss = 0.70550, time: 17.43 sec\n",
      "epoch : 446,train loss = 0.72935, test loss = 0.72133, running test loss = 0.70554, time: 17.49 sec\n",
      "epoch : 447,train loss = 0.68135, test loss = 0.71900, running test loss = 0.70557, time: 17.42 sec\n",
      "epoch : 448,train loss = 0.69205, test loss = 0.68727, running test loss = 0.70553, time: 17.41 sec\n",
      "epoch : 449,train loss = 0.69183, test loss = 0.87129, running test loss = 0.70590, time: 17.52 sec\n",
      "epoch : 450,train loss = 0.68456, test loss = 0.73289, running test loss = 0.70595, time: 17.42 sec\n",
      "epoch : 451,train loss = 0.66747, test loss = 0.66383, running test loss = 0.70586, time: 17.48 sec\n",
      "epoch : 452,train loss = 0.72752, test loss = 0.62155, running test loss = 0.70568, time: 17.47 sec\n",
      "epoch : 453,train loss = 0.69191, test loss = 0.65555, running test loss = 0.70557, time: 17.58 sec\n",
      "epoch : 454,train loss = 0.75571, test loss = 0.68838, running test loss = 0.70553, time: 17.45 sec\n",
      "epoch : 455,train loss = 0.68158, test loss = 0.67006, running test loss = 0.70545, time: 17.53 sec\n",
      "epoch : 456,train loss = 0.70520, test loss = 0.76747, running test loss = 0.70559, time: 17.48 sec\n",
      "epoch : 457,train loss = 0.71395, test loss = 0.71607, running test loss = 0.70561, time: 17.44 sec\n",
      "epoch : 458,train loss = 0.71848, test loss = 0.65424, running test loss = 0.70550, time: 17.61 sec\n",
      "epoch : 459,train loss = 0.69771, test loss = 0.65890, running test loss = 0.70539, time: 17.48 sec\n",
      "epoch : 460,train loss = 0.70344, test loss = 0.62130, running test loss = 0.70521, time: 17.47 sec\n",
      "epoch : 461,train loss = 0.70747, test loss = 0.76296, running test loss = 0.70534, time: 17.45 sec\n",
      "epoch : 462,train loss = 0.70663, test loss = 0.64869, running test loss = 0.70522, time: 17.51 sec\n",
      "epoch : 463,train loss = 0.69593, test loss = 0.64391, running test loss = 0.70508, time: 17.48 sec\n",
      "epoch : 464,train loss = 0.71108, test loss = 0.64050, running test loss = 0.70494, time: 17.57 sec\n",
      "epoch : 465,train loss = 0.74243, test loss = 0.72300, running test loss = 0.70498, time: 17.46 sec\n",
      "epoch : 466,train loss = 0.71314, test loss = 0.62119, running test loss = 0.70480, time: 17.52 sec\n",
      "epoch : 467,train loss = 0.69726, test loss = 0.67610, running test loss = 0.70474, time: 17.48 sec\n",
      "epoch : 468,train loss = 0.70193, test loss = 0.67147, running test loss = 0.70467, time: 17.49 sec\n",
      "epoch : 469,train loss = 0.70074, test loss = 0.69793, running test loss = 0.70466, time: 17.41 sec\n",
      "epoch : 470,train loss = 0.65551, test loss = 0.69719, running test loss = 0.70464, time: 17.49 sec\n",
      "epoch : 471,train loss = 0.70442, test loss = 0.77299, running test loss = 0.70479, time: 17.53 sec\n",
      "epoch : 472,train loss = 0.74539, test loss = 0.67852, running test loss = 0.70473, time: 17.47 sec\n",
      "epoch : 473,train loss = 0.71550, test loss = 0.69558, running test loss = 0.70471, time: 17.48 sec\n",
      "epoch : 474,train loss = 0.68505, test loss = 0.77746, running test loss = 0.70486, time: 17.53 sec\n",
      "epoch : 475,train loss = 0.68813, test loss = 0.84266, running test loss = 0.70515, time: 17.52 sec\n",
      "epoch : 476,train loss = 0.67242, test loss = 0.62726, running test loss = 0.70499, time: 17.39 sec\n",
      "epoch : 477,train loss = 0.70924, test loss = 0.67581, running test loss = 0.70493, time: 17.51 sec\n",
      "epoch : 478,train loss = 0.74051, test loss = 0.65164, running test loss = 0.70482, time: 17.50 sec\n",
      "epoch : 479,train loss = 0.71210, test loss = 0.66688, running test loss = 0.70474, time: 17.53 sec\n",
      "epoch : 480,train loss = 0.69765, test loss = 0.66140, running test loss = 0.70465, time: 17.48 sec\n",
      "epoch : 481,train loss = 0.72031, test loss = 0.71864, running test loss = 0.70468, time: 17.53 sec\n",
      "epoch : 482,train loss = 0.70477, test loss = 0.65375, running test loss = 0.70457, time: 17.55 sec\n",
      "epoch : 483,train loss = 0.68924, test loss = 0.77614, running test loss = 0.70472, time: 17.47 sec\n",
      "epoch : 484,train loss = 0.71575, test loss = 0.69476, running test loss = 0.70470, time: 17.51 sec\n",
      "epoch : 485,train loss = 0.70698, test loss = 0.67452, running test loss = 0.70464, time: 17.50 sec\n",
      "epoch : 486,train loss = 0.75779, test loss = 0.67941, running test loss = 0.70459, time: 17.47 sec\n",
      "epoch : 487,train loss = 0.68856, test loss = 0.70139, running test loss = 0.70458, time: 17.51 sec\n",
      "epoch : 488,train loss = 0.72529, test loss = 0.72908, running test loss = 0.70463, time: 17.53 sec\n",
      "epoch : 489,train loss = 0.69753, test loss = 0.73542, running test loss = 0.70469, time: 17.47 sec\n",
      "epoch : 490,train loss = 0.69848, test loss = 0.71626, running test loss = 0.70472, time: 17.51 sec\n",
      "epoch : 491,train loss = 0.67960, test loss = 0.72972, running test loss = 0.70477, time: 17.63 sec\n",
      "epoch : 492,train loss = 0.68667, test loss = 0.69023, running test loss = 0.70474, time: 17.45 sec\n",
      "epoch : 493,train loss = 0.68472, test loss = 0.68616, running test loss = 0.70470, time: 17.47 sec\n",
      "epoch : 494,train loss = 0.70432, test loss = 0.75153, running test loss = 0.70479, time: 17.46 sec\n",
      "epoch : 495,train loss = 0.70071, test loss = 0.59469, running test loss = 0.70457, time: 17.48 sec\n",
      "epoch : 496,train loss = 0.70581, test loss = 0.70144, running test loss = 0.70457, time: 17.47 sec\n",
      "epoch : 497,train loss = 0.69798, test loss = 0.68408, running test loss = 0.70452, time: 17.49 sec\n",
      "epoch : 498,train loss = 0.70163, test loss = 0.70586, running test loss = 0.70453, time: 17.39 sec\n",
      "epoch : 499,train loss = 0.66783, test loss = 0.71843, running test loss = 0.70456, time: 17.48 sec\n",
      "epoch : 500,train loss = 0.72098, test loss = 0.71931, running test loss = 0.70458, time: 17.55 sec\n",
      "epoch : 501,train loss = 0.71770, test loss = 0.78578, running test loss = 0.70475, time: 17.47 sec\n",
      "epoch : 502,train loss = 0.69327, test loss = 0.67230, running test loss = 0.70468, time: 17.56 sec\n",
      "epoch : 503,train loss = 0.72570, test loss = 0.64292, running test loss = 0.70456, time: 17.47 sec\n",
      "epoch : 504,train loss = 0.69870, test loss = 0.69817, running test loss = 0.70455, time: 17.51 sec\n",
      "epoch : 505,train loss = 0.71941, test loss = 0.67147, running test loss = 0.70448, time: 17.50 sec\n",
      "epoch : 506,train loss = 0.69106, test loss = 0.73870, running test loss = 0.70455, time: 17.45 sec\n",
      "epoch : 507,train loss = 0.71445, test loss = 0.74422, running test loss = 0.70463, time: 17.47 sec\n",
      "epoch : 508,train loss = 0.70119, test loss = 0.71268, running test loss = 0.70464, time: 17.50 sec\n",
      "epoch : 509,train loss = 0.69107, test loss = 0.75093, running test loss = 0.70473, time: 17.46 sec\n",
      "epoch : 510,train loss = 0.74095, test loss = 0.83126, running test loss = 0.70498, time: 17.51 sec\n",
      "epoch : 511,train loss = 0.71638, test loss = 0.69032, running test loss = 0.70495, time: 17.42 sec\n",
      "epoch : 512,train loss = 0.70414, test loss = 0.73186, running test loss = 0.70501, time: 17.56 sec\n",
      "epoch : 513,train loss = 0.69510, test loss = 0.72511, running test loss = 0.70504, time: 17.46 sec\n",
      "epoch : 514,train loss = 0.69734, test loss = 0.70290, running test loss = 0.70504, time: 17.44 sec\n",
      "epoch : 515,train loss = 0.67237, test loss = 0.77503, running test loss = 0.70518, time: 17.50 sec\n",
      "epoch : 516,train loss = 0.70066, test loss = 0.71612, running test loss = 0.70520, time: 17.46 sec\n",
      "epoch : 517,train loss = 0.70202, test loss = 0.69215, running test loss = 0.70517, time: 17.53 sec\n",
      "epoch : 518,train loss = 0.71647, test loss = 0.61867, running test loss = 0.70501, time: 17.49 sec\n",
      "epoch : 519,train loss = 0.68259, test loss = 0.69808, running test loss = 0.70499, time: 17.47 sec\n",
      "epoch : 520,train loss = 0.67921, test loss = 0.65636, running test loss = 0.70490, time: 17.47 sec\n",
      "epoch : 521,train loss = 0.69728, test loss = 0.75295, running test loss = 0.70499, time: 17.45 sec\n",
      "epoch : 522,train loss = 0.72801, test loss = 0.72404, running test loss = 0.70503, time: 17.50 sec\n",
      "epoch : 523,train loss = 0.69991, test loss = 0.67880, running test loss = 0.70498, time: 17.49 sec\n",
      "epoch : 524,train loss = 0.71574, test loss = 0.73827, running test loss = 0.70504, time: 17.42 sec\n",
      "epoch : 525,train loss = 0.71711, test loss = 0.78824, running test loss = 0.70520, time: 17.50 sec\n",
      "epoch : 526,train loss = 0.67370, test loss = 0.65435, running test loss = 0.70510, time: 17.49 sec\n",
      "epoch : 527,train loss = 0.70348, test loss = 0.66063, running test loss = 0.70502, time: 17.48 sec\n",
      "epoch : 528,train loss = 0.69115, test loss = 0.74312, running test loss = 0.70509, time: 17.50 sec\n",
      "epoch : 529,train loss = 0.69969, test loss = 0.72371, running test loss = 0.70512, time: 16.23 sec\n",
      "epoch : 530,train loss = 0.68117, test loss = 0.61749, running test loss = 0.70496, time: 15.79 sec\n",
      "epoch : 531,train loss = 0.68772, test loss = 0.76621, running test loss = 0.70508, time: 15.80 sec\n",
      "epoch : 532,train loss = 0.69817, test loss = 0.66963, running test loss = 0.70501, time: 15.81 sec\n",
      "epoch : 533,train loss = 0.67282, test loss = 0.76031, running test loss = 0.70511, time: 15.79 sec\n",
      "epoch : 534,train loss = 0.68053, test loss = 0.67224, running test loss = 0.70505, time: 15.81 sec\n",
      "epoch : 535,train loss = 0.68671, test loss = 0.70064, running test loss = 0.70504, time: 15.79 sec\n",
      "epoch : 536,train loss = 0.68658, test loss = 0.75098, running test loss = 0.70513, time: 15.79 sec\n",
      "epoch : 537,train loss = 0.68626, test loss = 0.70393, running test loss = 0.70513, time: 15.80 sec\n",
      "epoch : 538,train loss = 0.69350, test loss = 0.74940, running test loss = 0.70521, time: 15.80 sec\n",
      "epoch : 539,train loss = 0.72298, test loss = 0.70714, running test loss = 0.70521, time: 15.79 sec\n",
      "epoch : 540,train loss = 0.68088, test loss = 0.76077, running test loss = 0.70531, time: 15.80 sec\n",
      "epoch : 541,train loss = 0.71157, test loss = 0.77026, running test loss = 0.70543, time: 15.79 sec\n",
      "epoch : 542,train loss = 0.65471, test loss = 0.71754, running test loss = 0.70546, time: 15.79 sec\n",
      "epoch : 543,train loss = 0.68303, test loss = 0.67912, running test loss = 0.70541, time: 15.81 sec\n",
      "epoch : 544,train loss = 0.68270, test loss = 0.73686, running test loss = 0.70547, time: 15.81 sec\n",
      "epoch : 545,train loss = 0.69411, test loss = 0.81461, running test loss = 0.70567, time: 15.81 sec\n",
      "epoch : 546,train loss = 0.66901, test loss = 0.64343, running test loss = 0.70555, time: 15.81 sec\n",
      "epoch : 547,train loss = 0.68532, test loss = 0.65453, running test loss = 0.70546, time: 15.80 sec\n",
      "epoch : 548,train loss = 0.72334, test loss = 0.72655, running test loss = 0.70550, time: 15.80 sec\n",
      "epoch : 549,train loss = 0.69352, test loss = 0.81764, running test loss = 0.70570, time: 15.78 sec\n",
      "epoch : 550,train loss = 0.71039, test loss = 0.72996, running test loss = 0.70574, time: 15.78 sec\n",
      "epoch : 551,train loss = 0.68958, test loss = 0.69053, running test loss = 0.70572, time: 15.80 sec\n",
      "epoch : 552,train loss = 0.72331, test loss = 0.71025, running test loss = 0.70573, time: 15.81 sec\n",
      "epoch : 553,train loss = 0.68480, test loss = 0.77132, running test loss = 0.70584, time: 15.82 sec\n",
      "epoch : 554,train loss = 0.68742, test loss = 0.70286, running test loss = 0.70584, time: 15.80 sec\n",
      "epoch : 555,train loss = 0.67193, test loss = 0.63890, running test loss = 0.70572, time: 15.81 sec\n",
      "epoch : 556,train loss = 0.70660, test loss = 0.76024, running test loss = 0.70582, time: 15.82 sec\n",
      "epoch : 557,train loss = 0.68854, test loss = 0.68173, running test loss = 0.70577, time: 15.80 sec\n",
      "epoch : 558,train loss = 0.72168, test loss = 0.70482, running test loss = 0.70577, time: 15.80 sec\n",
      "epoch : 559,train loss = 0.72620, test loss = 0.68340, running test loss = 0.70573, time: 15.81 sec\n",
      "epoch : 560,train loss = 0.68311, test loss = 0.73467, running test loss = 0.70578, time: 15.79 sec\n",
      "epoch : 561,train loss = 0.71751, test loss = 0.67661, running test loss = 0.70573, time: 15.82 sec\n",
      "epoch : 562,train loss = 0.68974, test loss = 0.69587, running test loss = 0.70571, time: 15.81 sec\n",
      "epoch : 563,train loss = 0.66583, test loss = 0.67572, running test loss = 0.70566, time: 15.81 sec\n",
      "epoch : 564,train loss = 0.69197, test loss = 0.69400, running test loss = 0.70564, time: 15.81 sec\n",
      "epoch : 565,train loss = 0.73296, test loss = 0.69431, running test loss = 0.70562, time: 15.80 sec\n",
      "epoch : 566,train loss = 0.66662, test loss = 0.70011, running test loss = 0.70561, time: 15.79 sec\n",
      "epoch : 567,train loss = 0.69974, test loss = 0.76804, running test loss = 0.70572, time: 15.80 sec\n",
      "epoch : 568,train loss = 0.67873, test loss = 0.65505, running test loss = 0.70563, time: 15.80 sec\n",
      "epoch : 569,train loss = 0.68227, test loss = 0.75238, running test loss = 0.70571, time: 15.80 sec\n",
      "epoch : 570,train loss = 0.68789, test loss = 0.65835, running test loss = 0.70563, time: 15.81 sec\n",
      "epoch : 571,train loss = 0.66718, test loss = 0.72636, running test loss = 0.70567, time: 15.80 sec\n",
      "epoch : 572,train loss = 0.68963, test loss = 0.65142, running test loss = 0.70557, time: 15.80 sec\n",
      "epoch : 573,train loss = 0.72624, test loss = 0.64692, running test loss = 0.70547, time: 15.81 sec\n",
      "epoch : 574,train loss = 0.65704, test loss = 0.76328, running test loss = 0.70557, time: 15.71 sec\n",
      "epoch : 575,train loss = 0.63799, test loss = 0.72466, running test loss = 0.70560, time: 15.67 sec\n",
      "epoch : 576,train loss = 0.70480, test loss = 0.68506, running test loss = 0.70557, time: 15.68 sec\n",
      "epoch : 577,train loss = 0.68817, test loss = 0.61450, running test loss = 0.70541, time: 15.77 sec\n",
      "epoch : 578,train loss = 0.68404, test loss = 0.66700, running test loss = 0.70534, time: 15.80 sec\n",
      "epoch : 579,train loss = 0.71737, test loss = 0.62447, running test loss = 0.70520, time: 15.78 sec\n",
      "epoch : 580,train loss = 0.70024, test loss = 0.76441, running test loss = 0.70531, time: 15.76 sec\n",
      "epoch : 581,train loss = 0.70236, test loss = 0.74028, running test loss = 0.70537, time: 15.79 sec\n",
      "epoch : 582,train loss = 0.67456, test loss = 0.71500, running test loss = 0.70538, time: 15.80 sec\n",
      "epoch : 583,train loss = 0.68980, test loss = 0.69420, running test loss = 0.70536, time: 15.79 sec\n",
      "epoch : 584,train loss = 0.68029, test loss = 0.70427, running test loss = 0.70536, time: 15.80 sec\n",
      "epoch : 585,train loss = 0.72988, test loss = 0.66084, running test loss = 0.70529, time: 15.80 sec\n",
      "epoch : 586,train loss = 0.72017, test loss = 0.65170, running test loss = 0.70519, time: 15.79 sec\n",
      "epoch : 587,train loss = 0.66927, test loss = 0.75404, running test loss = 0.70528, time: 15.81 sec\n",
      "epoch : 588,train loss = 0.69372, test loss = 0.73057, running test loss = 0.70532, time: 15.79 sec\n",
      "epoch : 589,train loss = 0.65444, test loss = 0.64584, running test loss = 0.70522, time: 15.79 sec\n",
      "epoch : 590,train loss = 0.68397, test loss = 0.69533, running test loss = 0.70520, time: 15.79 sec\n",
      "epoch : 591,train loss = 0.70347, test loss = 0.72070, running test loss = 0.70523, time: 15.79 sec\n",
      "epoch : 592,train loss = 0.72692, test loss = 0.66280, running test loss = 0.70516, time: 15.79 sec\n",
      "epoch : 593,train loss = 0.64742, test loss = 0.66620, running test loss = 0.70509, time: 15.79 sec\n",
      "epoch : 594,train loss = 0.68044, test loss = 0.68181, running test loss = 0.70505, time: 15.79 sec\n",
      "epoch : 595,train loss = 0.70308, test loss = 0.71474, running test loss = 0.70507, time: 15.79 sec\n",
      "epoch : 596,train loss = 0.66878, test loss = 0.76776, running test loss = 0.70517, time: 15.79 sec\n",
      "epoch : 597,train loss = 0.73943, test loss = 0.72158, running test loss = 0.70520, time: 15.78 sec\n",
      "epoch : 598,train loss = 0.73505, test loss = 0.63320, running test loss = 0.70508, time: 15.80 sec\n",
      "epoch : 599,train loss = 0.71387, test loss = 0.70047, running test loss = 0.70507, time: 15.77 sec\n",
      "epoch : 600,train loss = 0.66679, test loss = 0.66394, running test loss = 0.70500, time: 15.79 sec\n",
      "epoch : 601,train loss = 0.69296, test loss = 0.74621, running test loss = 0.70507, time: 15.78 sec\n",
      "epoch : 602,train loss = 0.69783, test loss = 0.71098, running test loss = 0.70508, time: 15.76 sec\n",
      "epoch : 603,train loss = 0.68527, test loss = 0.70000, running test loss = 0.70507, time: 15.76 sec\n",
      "epoch : 604,train loss = 0.71850, test loss = 0.66496, running test loss = 0.70501, time: 15.79 sec\n",
      "epoch : 605,train loss = 0.69576, test loss = 0.77414, running test loss = 0.70512, time: 15.79 sec\n",
      "epoch : 606,train loss = 0.73252, test loss = 0.71749, running test loss = 0.70514, time: 15.78 sec\n",
      "epoch : 607,train loss = 0.70194, test loss = 0.68895, running test loss = 0.70512, time: 15.79 sec\n",
      "epoch : 608,train loss = 0.70110, test loss = 0.75155, running test loss = 0.70519, time: 15.79 sec\n",
      "epoch : 609,train loss = 0.65936, test loss = 0.66691, running test loss = 0.70513, time: 15.78 sec\n",
      "epoch : 610,train loss = 0.73011, test loss = 0.83409, running test loss = 0.70534, time: 15.78 sec\n",
      "epoch : 611,train loss = 0.73134, test loss = 0.65058, running test loss = 0.70525, time: 15.75 sec\n",
      "epoch : 612,train loss = 0.67747, test loss = 0.79703, running test loss = 0.70540, time: 15.79 sec\n",
      "epoch : 613,train loss = 0.70048, test loss = 0.65493, running test loss = 0.70532, time: 15.80 sec\n",
      "epoch : 614,train loss = 0.69718, test loss = 0.61633, running test loss = 0.70517, time: 15.79 sec\n",
      "epoch : 615,train loss = 0.71445, test loss = 0.63911, running test loss = 0.70507, time: 15.80 sec\n",
      "epoch : 616,train loss = 0.71533, test loss = 0.70826, running test loss = 0.70507, time: 15.78 sec\n",
      "epoch : 617,train loss = 0.72833, test loss = 0.71248, running test loss = 0.70508, time: 15.79 sec\n",
      "epoch : 618,train loss = 0.74222, test loss = 0.69836, running test loss = 0.70507, time: 15.79 sec\n",
      "epoch : 619,train loss = 0.67519, test loss = 0.73364, running test loss = 0.70512, time: 15.79 sec\n",
      "epoch : 620,train loss = 0.72112, test loss = 0.67306, running test loss = 0.70507, time: 15.79 sec\n",
      "epoch : 621,train loss = 0.72371, test loss = 0.67217, running test loss = 0.70501, time: 15.78 sec\n",
      "epoch : 622,train loss = 0.70209, test loss = 0.67091, running test loss = 0.70496, time: 15.79 sec\n",
      "epoch : 623,train loss = 0.65992, test loss = 0.68351, running test loss = 0.70493, time: 15.81 sec\n",
      "epoch : 624,train loss = 0.65692, test loss = 0.64390, running test loss = 0.70483, time: 15.79 sec\n",
      "epoch : 625,train loss = 0.72647, test loss = 0.69310, running test loss = 0.70481, time: 15.80 sec\n",
      "epoch : 626,train loss = 0.70293, test loss = 0.66338, running test loss = 0.70474, time: 15.80 sec\n",
      "epoch : 627,train loss = 0.69098, test loss = 0.74474, running test loss = 0.70481, time: 15.82 sec\n",
      "epoch : 628,train loss = 0.67053, test loss = 0.69290, running test loss = 0.70479, time: 15.80 sec\n",
      "epoch : 629,train loss = 0.67440, test loss = 0.64934, running test loss = 0.70470, time: 15.81 sec\n",
      "epoch : 630,train loss = 0.71835, test loss = 0.67179, running test loss = 0.70465, time: 15.80 sec\n",
      "epoch : 631,train loss = 0.70238, test loss = 0.79446, running test loss = 0.70479, time: 15.80 sec\n",
      "epoch : 632,train loss = 0.68620, test loss = 0.82856, running test loss = 0.70499, time: 15.80 sec\n",
      "epoch : 633,train loss = 0.70411, test loss = 0.59355, running test loss = 0.70481, time: 15.77 sec\n",
      "epoch : 634,train loss = 0.73480, test loss = 0.75647, running test loss = 0.70489, time: 15.78 sec\n",
      "epoch : 635,train loss = 0.68551, test loss = 0.72136, running test loss = 0.70492, time: 15.80 sec\n",
      "epoch : 636,train loss = 0.72386, test loss = 0.67010, running test loss = 0.70486, time: 15.79 sec\n",
      "epoch : 637,train loss = 0.70906, test loss = 0.69671, running test loss = 0.70485, time: 15.77 sec\n",
      "epoch : 638,train loss = 0.69041, test loss = 0.74949, running test loss = 0.70492, time: 15.77 sec\n",
      "epoch : 639,train loss = 0.68740, test loss = 0.66279, running test loss = 0.70485, time: 15.79 sec\n",
      "epoch : 640,train loss = 0.70604, test loss = 0.65390, running test loss = 0.70477, time: 15.79 sec\n",
      "epoch : 641,train loss = 0.70471, test loss = 0.72708, running test loss = 0.70481, time: 15.80 sec\n",
      "epoch : 642,train loss = 0.71120, test loss = 0.66343, running test loss = 0.70474, time: 15.79 sec\n",
      "epoch : 643,train loss = 0.69385, test loss = 0.76338, running test loss = 0.70484, time: 15.79 sec\n",
      "epoch : 644,train loss = 0.70911, test loss = 0.61642, running test loss = 0.70470, time: 15.79 sec\n",
      "epoch : 645,train loss = 0.70944, test loss = 0.76018, running test loss = 0.70478, time: 15.78 sec\n",
      "epoch : 646,train loss = 0.72550, test loss = 0.74042, running test loss = 0.70484, time: 15.79 sec\n",
      "epoch : 647,train loss = 0.70977, test loss = 0.64350, running test loss = 0.70474, time: 15.78 sec\n",
      "epoch : 648,train loss = 0.70162, test loss = 0.66871, running test loss = 0.70469, time: 15.78 sec\n",
      "epoch : 649,train loss = 0.69894, test loss = 0.76368, running test loss = 0.70478, time: 15.78 sec\n",
      "epoch : 650,train loss = 0.71814, test loss = 0.76354, running test loss = 0.70487, time: 15.79 sec\n",
      "epoch : 651,train loss = 0.72144, test loss = 0.70811, running test loss = 0.70488, time: 15.78 sec\n",
      "epoch : 652,train loss = 0.73955, test loss = 0.70166, running test loss = 0.70487, time: 15.79 sec\n",
      "epoch : 653,train loss = 0.71839, test loss = 0.67666, running test loss = 0.70483, time: 15.80 sec\n",
      "epoch : 654,train loss = 0.72895, test loss = 0.74757, running test loss = 0.70489, time: 15.80 sec\n",
      "epoch : 655,train loss = 0.70635, test loss = 0.69235, running test loss = 0.70487, time: 15.80 sec\n",
      "epoch : 656,train loss = 0.75951, test loss = 0.64351, running test loss = 0.70478, time: 15.80 sec\n",
      "epoch : 657,train loss = 0.71017, test loss = 0.75536, running test loss = 0.70486, time: 15.80 sec\n",
      "epoch : 658,train loss = 0.69773, test loss = 0.74881, running test loss = 0.70492, time: 15.78 sec\n",
      "epoch : 659,train loss = 0.69597, test loss = 0.76456, running test loss = 0.70501, time: 15.79 sec\n",
      "epoch : 660,train loss = 0.66026, test loss = 0.74212, running test loss = 0.70507, time: 15.79 sec\n",
      "epoch : 661,train loss = 0.70150, test loss = 0.75514, running test loss = 0.70515, time: 15.79 sec\n",
      "epoch : 662,train loss = 0.72506, test loss = 0.61009, running test loss = 0.70500, time: 15.79 sec\n",
      "epoch : 663,train loss = 0.72000, test loss = 0.73863, running test loss = 0.70505, time: 15.79 sec\n",
      "epoch : 664,train loss = 0.72770, test loss = 0.67967, running test loss = 0.70501, time: 15.79 sec\n",
      "epoch : 665,train loss = 0.73134, test loss = 0.71280, running test loss = 0.70503, time: 15.79 sec\n",
      "epoch : 666,train loss = 0.68582, test loss = 0.67669, running test loss = 0.70498, time: 15.78 sec\n",
      "epoch : 667,train loss = 0.71358, test loss = 0.83191, running test loss = 0.70517, time: 15.78 sec\n",
      "epoch : 668,train loss = 0.71480, test loss = 0.63440, running test loss = 0.70507, time: 15.79 sec\n",
      "epoch : 669,train loss = 0.70569, test loss = 0.67990, running test loss = 0.70503, time: 15.78 sec\n",
      "epoch : 670,train loss = 0.67772, test loss = 0.64672, running test loss = 0.70494, time: 15.78 sec\n",
      "epoch : 671,train loss = 0.71000, test loss = 0.66357, running test loss = 0.70488, time: 15.78 sec\n",
      "epoch : 672,train loss = 0.69554, test loss = 0.73516, running test loss = 0.70493, time: 15.78 sec\n",
      "epoch : 673,train loss = 0.71531, test loss = 0.71796, running test loss = 0.70495, time: 15.78 sec\n",
      "epoch : 674,train loss = 0.69196, test loss = 0.67505, running test loss = 0.70490, time: 15.78 sec\n",
      "epoch : 675,train loss = 0.68916, test loss = 0.64425, running test loss = 0.70481, time: 15.68 sec\n",
      "epoch : 676,train loss = 0.71183, test loss = 0.79480, running test loss = 0.70495, time: 15.65 sec\n",
      "epoch : 677,train loss = 0.69233, test loss = 0.64541, running test loss = 0.70486, time: 15.65 sec\n",
      "epoch : 678,train loss = 0.69046, test loss = 0.62303, running test loss = 0.70474, time: 15.65 sec\n",
      "epoch : 679,train loss = 0.71698, test loss = 0.65336, running test loss = 0.70466, time: 15.65 sec\n",
      "epoch : 680,train loss = 0.68493, test loss = 0.70972, running test loss = 0.70467, time: 15.66 sec\n",
      "epoch : 681,train loss = 0.67798, test loss = 0.77409, running test loss = 0.70477, time: 15.67 sec\n",
      "epoch : 682,train loss = 0.70166, test loss = 0.66697, running test loss = 0.70472, time: 15.63 sec\n",
      "epoch : 683,train loss = 0.69912, test loss = 0.68934, running test loss = 0.70469, time: 15.75 sec\n",
      "epoch : 684,train loss = 0.78362, test loss = 0.69235, running test loss = 0.70467, time: 15.79 sec\n",
      "epoch : 685,train loss = 0.69690, test loss = 0.63152, running test loss = 0.70457, time: 15.86 sec\n",
      "epoch : 686,train loss = 0.68262, test loss = 0.76132, running test loss = 0.70465, time: 15.80 sec\n",
      "epoch : 687,train loss = 0.70233, test loss = 0.75147, running test loss = 0.70472, time: 15.80 sec\n",
      "epoch : 688,train loss = 0.72316, test loss = 0.75317, running test loss = 0.70479, time: 15.86 sec\n",
      "epoch : 689,train loss = 0.68144, test loss = 0.73301, running test loss = 0.70483, time: 15.81 sec\n",
      "epoch : 690,train loss = 0.72348, test loss = 0.70116, running test loss = 0.70482, time: 15.81 sec\n",
      "epoch : 691,train loss = 0.70782, test loss = 0.68351, running test loss = 0.70479, time: 15.71 sec\n",
      "epoch : 692,train loss = 0.71208, test loss = 0.62044, running test loss = 0.70467, time: 20.30 sec\n",
      "epoch : 693,train loss = 0.72697, test loss = 0.71800, running test loss = 0.70469, time: 27.87 sec\n",
      "epoch : 694,train loss = 0.69996, test loss = 0.67369, running test loss = 0.70465, time: 23.92 sec\n",
      "epoch : 695,train loss = 0.69184, test loss = 0.67318, running test loss = 0.70460, time: 15.80 sec\n",
      "epoch : 696,train loss = 0.68794, test loss = 0.68334, running test loss = 0.70457, time: 15.84 sec\n",
      "epoch : 697,train loss = 0.69607, test loss = 0.76599, running test loss = 0.70466, time: 15.84 sec\n",
      "epoch : 698,train loss = 0.69359, test loss = 0.64038, running test loss = 0.70457, time: 15.82 sec\n",
      "epoch : 699,train loss = 0.68445, test loss = 0.61765, running test loss = 0.70444, time: 15.82 sec\n",
      "epoch : 700,train loss = 0.70928, test loss = 0.78567, running test loss = 0.70456, time: 19.09 sec\n",
      "epoch : 701,train loss = 0.71310, test loss = 0.70246, running test loss = 0.70456, time: 20.28 sec\n",
      "epoch : 702,train loss = 0.70026, test loss = 0.69770, running test loss = 0.70455, time: 15.95 sec\n",
      "epoch : 703,train loss = 0.70898, test loss = 0.66687, running test loss = 0.70449, time: 24.27 sec\n",
      "epoch : 704,train loss = 0.76054, test loss = 0.79727, running test loss = 0.70462, time: 24.78 sec\n",
      "epoch : 705,train loss = 0.69797, test loss = 0.65896, running test loss = 0.70456, time: 24.61 sec\n",
      "epoch : 706,train loss = 0.74641, test loss = 0.67664, running test loss = 0.70452, time: 24.46 sec\n",
      "epoch : 707,train loss = 0.69730, test loss = 0.63528, running test loss = 0.70442, time: 24.62 sec\n",
      "epoch : 708,train loss = 0.72576, test loss = 0.72852, running test loss = 0.70446, time: 24.36 sec\n",
      "epoch : 709,train loss = 0.69967, test loss = 0.73798, running test loss = 0.70450, time: 24.55 sec\n",
      "epoch : 710,train loss = 0.66726, test loss = 0.75155, running test loss = 0.70457, time: 24.64 sec\n",
      "epoch : 711,train loss = 0.67414, test loss = 0.71192, running test loss = 0.70458, time: 24.21 sec\n",
      "epoch : 712,train loss = 0.68319, test loss = 0.78537, running test loss = 0.70469, time: 24.82 sec\n",
      "epoch : 713,train loss = 0.67986, test loss = 0.71014, running test loss = 0.70470, time: 24.48 sec\n",
      "epoch : 714,train loss = 0.68449, test loss = 0.70015, running test loss = 0.70469, time: 24.67 sec\n",
      "epoch : 715,train loss = 0.67795, test loss = 0.73167, running test loss = 0.70473, time: 24.45 sec\n",
      "epoch : 716,train loss = 0.68040, test loss = 0.60391, running test loss = 0.70459, time: 24.50 sec\n",
      "epoch : 717,train loss = 0.72267, test loss = 0.72777, running test loss = 0.70462, time: 24.65 sec\n",
      "epoch : 718,train loss = 0.67706, test loss = 0.69347, running test loss = 0.70461, time: 24.09 sec\n",
      "epoch : 719,train loss = 0.67981, test loss = 0.63274, running test loss = 0.70451, time: 24.69 sec\n",
      "epoch : 720,train loss = 0.70740, test loss = 0.65700, running test loss = 0.70444, time: 24.53 sec\n",
      "epoch : 721,train loss = 0.71761, test loss = 0.61033, running test loss = 0.70431, time: 24.47 sec\n",
      "epoch : 722,train loss = 0.72702, test loss = 0.68496, running test loss = 0.70429, time: 24.31 sec\n",
      "epoch : 723,train loss = 0.67828, test loss = 0.68025, running test loss = 0.70425, time: 24.70 sec\n",
      "epoch : 724,train loss = 0.73857, test loss = 0.71766, running test loss = 0.70427, time: 24.38 sec\n",
      "epoch : 725,train loss = 0.69311, test loss = 0.66096, running test loss = 0.70421, time: 24.51 sec\n",
      "epoch : 726,train loss = 0.65647, test loss = 0.61536, running test loss = 0.70409, time: 24.51 sec\n",
      "epoch : 727,train loss = 0.70399, test loss = 0.62531, running test loss = 0.70398, time: 24.75 sec\n",
      "epoch : 728,train loss = 0.72981, test loss = 0.69990, running test loss = 0.70397, time: 24.52 sec\n",
      "epoch : 729,train loss = 0.70221, test loss = 0.67124, running test loss = 0.70393, time: 24.49 sec\n",
      "epoch : 730,train loss = 0.73455, test loss = 0.67226, running test loss = 0.70389, time: 24.46 sec\n",
      "epoch : 731,train loss = 0.72286, test loss = 0.72162, running test loss = 0.70391, time: 24.59 sec\n",
      "epoch : 732,train loss = 0.66406, test loss = 0.74151, running test loss = 0.70396, time: 24.68 sec\n",
      "epoch : 733,train loss = 0.67891, test loss = 0.70604, running test loss = 0.70397, time: 24.55 sec\n",
      "epoch : 734,train loss = 0.67269, test loss = 0.60025, running test loss = 0.70382, time: 24.16 sec\n",
      "epoch : 735,train loss = 0.70872, test loss = 0.68650, running test loss = 0.70380, time: 24.32 sec\n",
      "epoch : 736,train loss = 0.69713, test loss = 0.66787, running test loss = 0.70375, time: 24.32 sec\n",
      "epoch : 737,train loss = 0.68821, test loss = 0.67192, running test loss = 0.70371, time: 24.54 sec\n",
      "epoch : 738,train loss = 0.69580, test loss = 0.72933, running test loss = 0.70374, time: 24.48 sec\n",
      "epoch : 739,train loss = 0.70299, test loss = 0.63896, running test loss = 0.70366, time: 24.41 sec\n",
      "epoch : 740,train loss = 0.70082, test loss = 0.68794, running test loss = 0.70363, time: 24.31 sec\n",
      "epoch : 741,train loss = 0.66965, test loss = 0.63251, running test loss = 0.70354, time: 24.61 sec\n",
      "epoch : 742,train loss = 0.67023, test loss = 0.67042, running test loss = 0.70349, time: 24.74 sec\n",
      "epoch : 743,train loss = 0.68622, test loss = 0.73671, running test loss = 0.70354, time: 24.37 sec\n",
      "epoch : 744,train loss = 0.69191, test loss = 0.67829, running test loss = 0.70350, time: 24.74 sec\n",
      "epoch : 745,train loss = 0.69243, test loss = 0.76493, running test loss = 0.70359, time: 24.13 sec\n",
      "epoch : 746,train loss = 0.69495, test loss = 0.83131, running test loss = 0.70376, time: 24.67 sec\n",
      "epoch : 747,train loss = 0.70452, test loss = 0.75093, running test loss = 0.70382, time: 24.50 sec\n",
      "epoch : 748,train loss = 0.69593, test loss = 0.77600, running test loss = 0.70392, time: 24.64 sec\n",
      "epoch : 749,train loss = 0.66648, test loss = 0.66540, running test loss = 0.70387, time: 24.35 sec\n",
      "epoch : 750,train loss = 0.67648, test loss = 0.71420, running test loss = 0.70388, time: 24.51 sec\n",
      "epoch : 751,train loss = 0.69830, test loss = 0.72113, running test loss = 0.70390, time: 24.79 sec\n",
      "epoch : 752,train loss = 0.68726, test loss = 0.78720, running test loss = 0.70401, time: 24.73 sec\n",
      "epoch : 753,train loss = 0.69770, test loss = 0.59744, running test loss = 0.70387, time: 24.66 sec\n",
      "epoch : 754,train loss = 0.71713, test loss = 0.83738, running test loss = 0.70405, time: 24.38 sec\n",
      "epoch : 755,train loss = 0.69260, test loss = 0.77315, running test loss = 0.70414, time: 24.76 sec\n",
      "epoch : 756,train loss = 0.71062, test loss = 0.68737, running test loss = 0.70412, time: 24.52 sec\n",
      "epoch : 757,train loss = 0.69391, test loss = 0.66005, running test loss = 0.70406, time: 24.35 sec\n",
      "epoch : 758,train loss = 0.69574, test loss = 0.68303, running test loss = 0.70403, time: 24.62 sec\n",
      "epoch : 759,train loss = 0.69597, test loss = 0.68980, running test loss = 0.70401, time: 24.38 sec\n",
      "epoch : 760,train loss = 0.69653, test loss = 0.68226, running test loss = 0.70399, time: 24.59 sec\n",
      "epoch : 761,train loss = 0.71907, test loss = 0.70395, running test loss = 0.70399, time: 24.41 sec\n",
      "epoch : 762,train loss = 0.68472, test loss = 0.60178, running test loss = 0.70385, time: 24.58 sec\n",
      "epoch : 763,train loss = 0.68370, test loss = 0.69741, running test loss = 0.70384, time: 24.44 sec\n",
      "epoch : 764,train loss = 0.69373, test loss = 0.56407, running test loss = 0.70366, time: 24.87 sec\n",
      "epoch : 765,train loss = 0.65974, test loss = 0.73426, running test loss = 0.70370, time: 24.67 sec\n",
      "epoch : 766,train loss = 0.67075, test loss = 0.67669, running test loss = 0.70366, time: 24.45 sec\n",
      "epoch : 767,train loss = 0.70274, test loss = 0.76635, running test loss = 0.70375, time: 24.79 sec\n",
      "epoch : 768,train loss = 0.69454, test loss = 0.70562, running test loss = 0.70375, time: 24.59 sec\n",
      "epoch : 769,train loss = 0.70932, test loss = 0.73416, running test loss = 0.70379, time: 24.36 sec\n",
      "epoch : 770,train loss = 0.68224, test loss = 0.63674, running test loss = 0.70370, time: 24.40 sec\n",
      "epoch : 771,train loss = 0.66927, test loss = 0.65980, running test loss = 0.70364, time: 24.44 sec\n",
      "epoch : 772,train loss = 0.69746, test loss = 0.72888, running test loss = 0.70368, time: 24.31 sec\n",
      "epoch : 773,train loss = 0.74008, test loss = 0.72340, running test loss = 0.70370, time: 24.67 sec\n",
      "epoch : 774,train loss = 0.71916, test loss = 0.73082, running test loss = 0.70374, time: 24.46 sec\n",
      "epoch : 775,train loss = 0.70121, test loss = 0.66350, running test loss = 0.70369, time: 24.24 sec\n",
      "epoch : 776,train loss = 0.70244, test loss = 0.65476, running test loss = 0.70362, time: 18.17 sec\n",
      "epoch : 777,train loss = 0.71526, test loss = 0.67115, running test loss = 0.70358, time: 21.42 sec\n",
      "epoch : 778,train loss = 0.72768, test loss = 0.65287, running test loss = 0.70352, time: 15.78 sec\n",
      "epoch : 779,train loss = 0.69431, test loss = 0.73580, running test loss = 0.70356, time: 15.72 sec\n",
      "epoch : 780,train loss = 0.74050, test loss = 0.72609, running test loss = 0.70359, time: 16.47 sec\n",
      "epoch : 781,train loss = 0.64433, test loss = 0.74851, running test loss = 0.70364, time: 24.49 sec\n",
      "epoch : 782,train loss = 0.72097, test loss = 0.75815, running test loss = 0.70371, time: 24.21 sec\n",
      "epoch : 783,train loss = 0.66967, test loss = 0.69192, running test loss = 0.70370, time: 24.27 sec\n",
      "epoch : 784,train loss = 0.68885, test loss = 0.72074, running test loss = 0.70372, time: 24.20 sec\n",
      "epoch : 785,train loss = 0.68850, test loss = 0.72235, running test loss = 0.70374, time: 24.53 sec\n",
      "epoch : 786,train loss = 0.66066, test loss = 0.78733, running test loss = 0.70385, time: 24.21 sec\n",
      "epoch : 787,train loss = 0.68489, test loss = 0.58863, running test loss = 0.70370, time: 24.47 sec\n",
      "epoch : 788,train loss = 0.71199, test loss = 0.71912, running test loss = 0.70372, time: 24.37 sec\n",
      "epoch : 789,train loss = 0.65304, test loss = 0.73270, running test loss = 0.70376, time: 24.22 sec\n",
      "epoch : 790,train loss = 0.68060, test loss = 0.75675, running test loss = 0.70383, time: 23.57 sec\n",
      "epoch : 791,train loss = 0.68175, test loss = 0.65940, running test loss = 0.70377, time: 24.68 sec\n",
      "epoch : 792,train loss = 0.67545, test loss = 0.73443, running test loss = 0.70381, time: 24.55 sec\n",
      "epoch : 793,train loss = 0.70110, test loss = 0.73401, running test loss = 0.70385, time: 24.65 sec\n",
      "epoch : 794,train loss = 0.68111, test loss = 0.64126, running test loss = 0.70377, time: 24.50 sec\n",
      "epoch : 795,train loss = 0.68565, test loss = 0.73952, running test loss = 0.70381, time: 24.06 sec\n",
      "epoch : 796,train loss = 0.71085, test loss = 0.77278, running test loss = 0.70390, time: 24.54 sec\n",
      "epoch : 797,train loss = 0.74161, test loss = 0.67638, running test loss = 0.70387, time: 24.53 sec\n",
      "epoch : 798,train loss = 0.71763, test loss = 0.65301, running test loss = 0.70380, time: 24.34 sec\n",
      "epoch : 799,train loss = 0.67051, test loss = 0.73243, running test loss = 0.70384, time: 24.35 sec\n",
      "epoch : 800,train loss = 0.72078, test loss = 0.63135, running test loss = 0.70375, time: 24.21 sec\n",
      "epoch : 801,train loss = 0.66420, test loss = 0.73139, running test loss = 0.70378, time: 24.35 sec\n",
      "epoch : 802,train loss = 0.68986, test loss = 0.67311, running test loss = 0.70374, time: 24.57 sec\n",
      "epoch : 803,train loss = 0.70778, test loss = 0.68707, running test loss = 0.70372, time: 24.44 sec\n",
      "epoch : 804,train loss = 0.68488, test loss = 0.85520, running test loss = 0.70391, time: 24.51 sec\n",
      "epoch : 805,train loss = 0.69958, test loss = 0.66543, running test loss = 0.70386, time: 23.82 sec\n",
      "epoch : 806,train loss = 0.72783, test loss = 0.67856, running test loss = 0.70383, time: 23.59 sec\n",
      "epoch : 807,train loss = 0.71711, test loss = 0.81420, running test loss = 0.70397, time: 24.56 sec\n",
      "epoch : 808,train loss = 0.69016, test loss = 0.75218, running test loss = 0.70403, time: 24.42 sec\n",
      "epoch : 809,train loss = 0.72034, test loss = 0.68364, running test loss = 0.70400, time: 24.38 sec\n",
      "epoch : 810,train loss = 0.71934, test loss = 0.78011, running test loss = 0.70410, time: 24.47 sec\n",
      "epoch : 811,train loss = 0.71269, test loss = 0.69889, running test loss = 0.70409, time: 24.16 sec\n",
      "epoch : 812,train loss = 0.66329, test loss = 0.63977, running test loss = 0.70401, time: 24.68 sec\n",
      "epoch : 813,train loss = 0.70357, test loss = 0.65942, running test loss = 0.70396, time: 23.48 sec\n",
      "epoch : 814,train loss = 0.69767, test loss = 0.81700, running test loss = 0.70410, time: 24.20 sec\n",
      "epoch : 815,train loss = 0.69774, test loss = 0.75190, running test loss = 0.70415, time: 24.44 sec\n",
      "epoch : 816,train loss = 0.69861, test loss = 0.74727, running test loss = 0.70421, time: 24.37 sec\n",
      "epoch : 817,train loss = 0.70787, test loss = 0.78909, running test loss = 0.70431, time: 24.11 sec\n",
      "epoch : 818,train loss = 0.72620, test loss = 0.74670, running test loss = 0.70436, time: 23.82 sec\n",
      "epoch : 819,train loss = 0.73656, test loss = 0.65551, running test loss = 0.70430, time: 23.25 sec\n",
      "epoch : 820,train loss = 0.69576, test loss = 0.73263, running test loss = 0.70434, time: 24.48 sec\n",
      "epoch : 821,train loss = 0.70109, test loss = 0.75563, running test loss = 0.70440, time: 23.65 sec\n",
      "epoch : 822,train loss = 0.70211, test loss = 0.67512, running test loss = 0.70436, time: 24.31 sec\n",
      "epoch : 823,train loss = 0.70334, test loss = 0.65318, running test loss = 0.70430, time: 23.96 sec\n",
      "epoch : 824,train loss = 0.70189, test loss = 0.73781, running test loss = 0.70434, time: 24.49 sec\n",
      "epoch : 825,train loss = 0.68941, test loss = 0.75569, running test loss = 0.70440, time: 24.37 sec\n",
      "epoch : 826,train loss = 0.71075, test loss = 0.70565, running test loss = 0.70441, time: 23.80 sec\n",
      "epoch : 827,train loss = 0.68210, test loss = 0.74442, running test loss = 0.70445, time: 24.69 sec\n",
      "epoch : 828,train loss = 0.70569, test loss = 0.64419, running test loss = 0.70438, time: 24.49 sec\n",
      "epoch : 829,train loss = 0.65218, test loss = 0.69048, running test loss = 0.70436, time: 24.13 sec\n",
      "epoch : 830,train loss = 0.70869, test loss = 0.75247, running test loss = 0.70442, time: 23.50 sec\n",
      "epoch : 831,train loss = 0.68431, test loss = 0.76645, running test loss = 0.70450, time: 23.54 sec\n",
      "epoch : 832,train loss = 0.71056, test loss = 0.76828, running test loss = 0.70457, time: 24.53 sec\n",
      "epoch : 833,train loss = 0.70076, test loss = 0.75364, running test loss = 0.70463, time: 23.79 sec\n",
      "epoch : 834,train loss = 0.71128, test loss = 0.69412, running test loss = 0.70462, time: 24.13 sec\n",
      "epoch : 835,train loss = 0.67287, test loss = 0.67561, running test loss = 0.70459, time: 23.67 sec\n",
      "epoch : 836,train loss = 0.68952, test loss = 0.69659, running test loss = 0.70458, time: 23.68 sec\n",
      "epoch : 837,train loss = 0.71540, test loss = 0.71522, running test loss = 0.70459, time: 24.26 sec\n",
      "epoch : 838,train loss = 0.69683, test loss = 0.70358, running test loss = 0.70459, time: 23.98 sec\n",
      "epoch : 839,train loss = 0.71674, test loss = 0.65386, running test loss = 0.70453, time: 24.37 sec\n",
      "epoch : 840,train loss = 0.68841, test loss = 0.67534, running test loss = 0.70449, time: 24.40 sec\n",
      "epoch : 841,train loss = 0.67122, test loss = 0.70022, running test loss = 0.70449, time: 24.45 sec\n",
      "epoch : 842,train loss = 0.65999, test loss = 0.72567, running test loss = 0.70451, time: 24.22 sec\n",
      "epoch : 843,train loss = 0.72000, test loss = 0.72008, running test loss = 0.70453, time: 24.32 sec\n",
      "epoch : 844,train loss = 0.66329, test loss = 0.86563, running test loss = 0.70472, time: 24.33 sec\n",
      "epoch : 845,train loss = 0.66965, test loss = 0.78153, running test loss = 0.70481, time: 24.58 sec\n",
      "epoch : 846,train loss = 0.67112, test loss = 0.71999, running test loss = 0.70483, time: 24.48 sec\n",
      "epoch : 847,train loss = 0.65959, test loss = 0.62549, running test loss = 0.70474, time: 24.08 sec\n",
      "epoch : 848,train loss = 0.70490, test loss = 0.71644, running test loss = 0.70475, time: 24.33 sec\n",
      "epoch : 849,train loss = 0.66199, test loss = 0.71499, running test loss = 0.70476, time: 24.63 sec\n",
      "epoch : 850,train loss = 0.70040, test loss = 0.69464, running test loss = 0.70475, time: 24.32 sec\n",
      "epoch : 851,train loss = 0.68408, test loss = 0.70565, running test loss = 0.70475, time: 24.58 sec\n",
      "epoch : 852,train loss = 0.69070, test loss = 0.76810, running test loss = 0.70483, time: 24.18 sec\n",
      "epoch : 853,train loss = 0.71790, test loss = 0.71122, running test loss = 0.70483, time: 24.13 sec\n",
      "epoch : 854,train loss = 0.70064, test loss = 0.70113, running test loss = 0.70483, time: 24.60 sec\n",
      "epoch : 855,train loss = 0.68751, test loss = 0.73977, running test loss = 0.70487, time: 24.27 sec\n",
      "epoch : 856,train loss = 0.63649, test loss = 0.74488, running test loss = 0.70492, time: 24.36 sec\n",
      "epoch : 857,train loss = 0.69587, test loss = 0.68951, running test loss = 0.70490, time: 24.33 sec\n",
      "epoch : 858,train loss = 0.71119, test loss = 0.60313, running test loss = 0.70478, time: 24.48 sec\n",
      "epoch : 859,train loss = 0.72384, test loss = 0.72881, running test loss = 0.70481, time: 24.23 sec\n",
      "epoch : 860,train loss = 0.65285, test loss = 0.78246, running test loss = 0.70490, time: 24.02 sec\n",
      "epoch : 861,train loss = 0.68664, test loss = 0.75996, running test loss = 0.70496, time: 18.65 sec\n",
      "epoch : 862,train loss = 0.68983, test loss = 0.66911, running test loss = 0.70492, time: 15.60 sec\n",
      "epoch : 863,train loss = 0.72007, test loss = 0.70400, running test loss = 0.70492, time: 15.54 sec\n",
      "epoch : 864,train loss = 0.71523, test loss = 0.69533, running test loss = 0.70491, time: 15.54 sec\n",
      "epoch : 865,train loss = 0.67482, test loss = 0.69895, running test loss = 0.70490, time: 15.55 sec\n",
      "epoch : 866,train loss = 0.69723, test loss = 0.70071, running test loss = 0.70490, time: 15.53 sec\n",
      "epoch : 867,train loss = 0.69708, test loss = 0.69222, running test loss = 0.70488, time: 15.51 sec\n",
      "epoch : 868,train loss = 0.65593, test loss = 0.70440, running test loss = 0.70488, time: 15.51 sec\n",
      "epoch : 869,train loss = 0.71598, test loss = 0.65494, running test loss = 0.70482, time: 15.51 sec\n",
      "epoch : 870,train loss = 0.65884, test loss = 0.70971, running test loss = 0.70483, time: 15.52 sec\n",
      "epoch : 871,train loss = 0.69489, test loss = 0.71710, running test loss = 0.70484, time: 15.48 sec\n",
      "epoch : 872,train loss = 0.69095, test loss = 0.70356, running test loss = 0.70484, time: 15.47 sec\n",
      "epoch : 873,train loss = 0.70372, test loss = 0.79438, running test loss = 0.70494, time: 15.51 sec\n",
      "epoch : 874,train loss = 0.72239, test loss = 0.65477, running test loss = 0.70489, time: 15.55 sec\n",
      "epoch : 875,train loss = 0.71815, test loss = 0.74765, running test loss = 0.70494, time: 15.56 sec\n",
      "epoch : 876,train loss = 0.66244, test loss = 0.67850, running test loss = 0.70491, time: 15.57 sec\n",
      "epoch : 877,train loss = 0.69154, test loss = 0.67452, running test loss = 0.70487, time: 15.56 sec\n",
      "epoch : 878,train loss = 0.69437, test loss = 0.66399, running test loss = 0.70482, time: 15.55 sec\n",
      "epoch : 879,train loss = 0.70034, test loss = 0.72860, running test loss = 0.70485, time: 15.56 sec\n",
      "epoch : 880,train loss = 0.68756, test loss = 0.71923, running test loss = 0.70487, time: 15.57 sec\n",
      "epoch : 881,train loss = 0.68697, test loss = 0.74024, running test loss = 0.70491, time: 15.56 sec\n",
      "epoch : 882,train loss = 0.69845, test loss = 0.68168, running test loss = 0.70488, time: 15.56 sec\n",
      "epoch : 883,train loss = 0.68454, test loss = 0.63999, running test loss = 0.70481, time: 15.56 sec\n",
      "epoch : 884,train loss = 0.73044, test loss = 0.73069, running test loss = 0.70484, time: 15.55 sec\n",
      "epoch : 885,train loss = 0.68274, test loss = 0.76003, running test loss = 0.70490, time: 15.54 sec\n",
      "epoch : 886,train loss = 0.74031, test loss = 0.66286, running test loss = 0.70485, time: 15.56 sec\n",
      "epoch : 887,train loss = 0.69633, test loss = 0.76172, running test loss = 0.70492, time: 15.56 sec\n",
      "epoch : 888,train loss = 0.72182, test loss = 0.64829, running test loss = 0.70485, time: 15.55 sec\n",
      "epoch : 889,train loss = 0.69917, test loss = 0.67835, running test loss = 0.70482, time: 15.55 sec\n",
      "epoch : 890,train loss = 0.71777, test loss = 0.74057, running test loss = 0.70486, time: 15.57 sec\n",
      "epoch : 891,train loss = 0.72135, test loss = 0.57190, running test loss = 0.70471, time: 15.56 sec\n",
      "epoch : 892,train loss = 0.72423, test loss = 0.70659, running test loss = 0.70472, time: 15.54 sec\n",
      "epoch : 893,train loss = 0.72140, test loss = 0.69465, running test loss = 0.70471, time: 15.55 sec\n",
      "epoch : 894,train loss = 0.72114, test loss = 0.80969, running test loss = 0.70482, time: 15.57 sec\n",
      "epoch : 895,train loss = 0.69097, test loss = 0.68788, running test loss = 0.70480, time: 15.56 sec\n",
      "epoch : 896,train loss = 0.73124, test loss = 0.62332, running test loss = 0.70471, time: 15.55 sec\n",
      "epoch : 897,train loss = 0.68282, test loss = 0.65218, running test loss = 0.70465, time: 15.57 sec\n",
      "epoch : 898,train loss = 0.68702, test loss = 0.69394, running test loss = 0.70464, time: 15.57 sec\n",
      "epoch : 899,train loss = 0.70375, test loss = 0.71851, running test loss = 0.70466, time: 15.55 sec\n",
      "epoch : 900,train loss = 0.72167, test loss = 0.73766, running test loss = 0.70469, time: 15.55 sec\n",
      "epoch : 901,train loss = 0.76080, test loss = 0.67138, running test loss = 0.70466, time: 15.55 sec\n",
      "epoch : 902,train loss = 0.68356, test loss = 0.65236, running test loss = 0.70460, time: 15.54 sec\n",
      "epoch : 903,train loss = 0.67530, test loss = 0.68684, running test loss = 0.70458, time: 15.55 sec\n",
      "epoch : 904,train loss = 0.69720, test loss = 0.82223, running test loss = 0.70471, time: 15.56 sec\n",
      "epoch : 905,train loss = 0.70820, test loss = 0.70203, running test loss = 0.70471, time: 15.55 sec\n",
      "epoch : 906,train loss = 0.71323, test loss = 0.65880, running test loss = 0.70466, time: 15.55 sec\n",
      "epoch : 907,train loss = 0.66908, test loss = 0.68987, running test loss = 0.70464, time: 15.54 sec\n",
      "epoch : 908,train loss = 0.70949, test loss = 0.67501, running test loss = 0.70461, time: 15.55 sec\n",
      "epoch : 909,train loss = 0.71866, test loss = 0.71879, running test loss = 0.70462, time: 15.55 sec\n",
      "epoch : 910,train loss = 0.73375, test loss = 0.71320, running test loss = 0.70463, time: 15.55 sec\n",
      "epoch : 911,train loss = 0.68275, test loss = 0.68929, running test loss = 0.70462, time: 15.55 sec\n",
      "epoch : 912,train loss = 0.69569, test loss = 0.57329, running test loss = 0.70447, time: 15.52 sec\n",
      "epoch : 913,train loss = 0.72000, test loss = 0.75681, running test loss = 0.70453, time: 15.42 sec\n",
      "epoch : 914,train loss = 0.70541, test loss = 0.69528, running test loss = 0.70452, time: 15.44 sec\n",
      "epoch : 915,train loss = 0.74391, test loss = 0.62332, running test loss = 0.70443, time: 15.43 sec\n",
      "epoch : 916,train loss = 0.71401, test loss = 0.67322, running test loss = 0.70440, time: 15.53 sec\n",
      "epoch : 917,train loss = 0.71761, test loss = 0.70469, running test loss = 0.70440, time: 15.54 sec\n",
      "epoch : 918,train loss = 0.68562, test loss = 0.67219, running test loss = 0.70436, time: 15.44 sec\n",
      "epoch : 919,train loss = 0.71515, test loss = 0.69821, running test loss = 0.70435, time: 15.43 sec\n",
      "epoch : 920,train loss = 0.67340, test loss = 0.66308, running test loss = 0.70431, time: 15.41 sec\n",
      "epoch : 921,train loss = 0.67061, test loss = 0.65758, running test loss = 0.70426, time: 15.44 sec\n",
      "epoch : 922,train loss = 0.70624, test loss = 0.67709, running test loss = 0.70423, time: 15.43 sec\n",
      "epoch : 923,train loss = 0.73479, test loss = 0.80198, running test loss = 0.70434, time: 15.42 sec\n",
      "epoch : 924,train loss = 0.68614, test loss = 0.68294, running test loss = 0.70431, time: 15.43 sec\n",
      "epoch : 925,train loss = 0.66946, test loss = 0.72538, running test loss = 0.70434, time: 15.42 sec\n",
      "epoch : 926,train loss = 0.71318, test loss = 0.68200, running test loss = 0.70431, time: 15.43 sec\n",
      "epoch : 927,train loss = 0.70932, test loss = 0.64514, running test loss = 0.70425, time: 15.44 sec\n",
      "epoch : 928,train loss = 0.70916, test loss = 0.65198, running test loss = 0.70419, time: 15.46 sec\n",
      "epoch : 929,train loss = 0.68837, test loss = 0.61444, running test loss = 0.70409, time: 15.45 sec\n",
      "epoch : 930,train loss = 0.73296, test loss = 0.75223, running test loss = 0.70415, time: 15.43 sec\n",
      "epoch : 931,train loss = 0.71120, test loss = 0.69920, running test loss = 0.70414, time: 15.44 sec\n",
      "epoch : 932,train loss = 0.75420, test loss = 0.63920, running test loss = 0.70407, time: 15.49 sec\n",
      "epoch : 933,train loss = 0.70262, test loss = 0.76193, running test loss = 0.70413, time: 15.57 sec\n",
      "epoch : 934,train loss = 0.72006, test loss = 0.67976, running test loss = 0.70411, time: 15.56 sec\n",
      "epoch : 935,train loss = 0.70589, test loss = 0.73106, running test loss = 0.70414, time: 15.56 sec\n",
      "epoch : 936,train loss = 0.70662, test loss = 0.68676, running test loss = 0.70412, time: 15.56 sec\n",
      "epoch : 937,train loss = 0.73542, test loss = 0.70959, running test loss = 0.70412, time: 15.56 sec\n",
      "epoch : 938,train loss = 0.67895, test loss = 0.60418, running test loss = 0.70402, time: 15.57 sec\n",
      "epoch : 939,train loss = 0.66125, test loss = 0.65245, running test loss = 0.70396, time: 15.56 sec\n",
      "epoch : 940,train loss = 0.67472, test loss = 0.72105, running test loss = 0.70398, time: 15.56 sec\n",
      "epoch : 941,train loss = 0.72303, test loss = 0.81097, running test loss = 0.70409, time: 15.57 sec\n",
      "epoch : 942,train loss = 0.68883, test loss = 0.70390, running test loss = 0.70409, time: 15.56 sec\n",
      "epoch : 943,train loss = 0.68402, test loss = 0.66594, running test loss = 0.70405, time: 15.56 sec\n",
      "epoch : 944,train loss = 0.71492, test loss = 0.72298, running test loss = 0.70407, time: 15.55 sec\n",
      "epoch : 945,train loss = 0.67630, test loss = 0.68984, running test loss = 0.70406, time: 15.57 sec\n",
      "epoch : 946,train loss = 0.70076, test loss = 0.85535, running test loss = 0.70422, time: 15.57 sec\n",
      "epoch : 947,train loss = 0.72824, test loss = 0.67800, running test loss = 0.70419, time: 15.56 sec\n",
      "epoch : 948,train loss = 0.69582, test loss = 0.77849, running test loss = 0.70427, time: 15.55 sec\n",
      "epoch : 949,train loss = 0.69329, test loss = 0.66305, running test loss = 0.70423, time: 15.56 sec\n",
      "epoch : 950,train loss = 0.65438, test loss = 0.72241, running test loss = 0.70424, time: 15.56 sec\n",
      "epoch : 951,train loss = 0.67860, test loss = 0.74260, running test loss = 0.70428, time: 15.57 sec\n",
      "epoch : 952,train loss = 0.68409, test loss = 0.70511, running test loss = 0.70429, time: 15.57 sec\n",
      "epoch : 953,train loss = 0.66177, test loss = 0.65024, running test loss = 0.70423, time: 15.56 sec\n",
      "epoch : 954,train loss = 0.70178, test loss = 0.68421, running test loss = 0.70421, time: 15.56 sec\n",
      "epoch : 955,train loss = 0.71824, test loss = 0.70207, running test loss = 0.70421, time: 15.56 sec\n",
      "epoch : 956,train loss = 0.67461, test loss = 0.80137, running test loss = 0.70431, time: 15.57 sec\n",
      "epoch : 957,train loss = 0.70438, test loss = 0.76892, running test loss = 0.70437, time: 15.56 sec\n",
      "epoch : 958,train loss = 0.71214, test loss = 0.75835, running test loss = 0.70443, time: 15.56 sec\n",
      "epoch : 959,train loss = 0.67423, test loss = 0.68263, running test loss = 0.70441, time: 15.56 sec\n",
      "epoch : 960,train loss = 0.67738, test loss = 0.62775, running test loss = 0.70433, time: 15.58 sec\n",
      "epoch : 961,train loss = 0.71572, test loss = 0.66589, running test loss = 0.70429, time: 15.56 sec\n",
      "epoch : 962,train loss = 0.69895, test loss = 0.64724, running test loss = 0.70423, time: 15.58 sec\n",
      "epoch : 963,train loss = 0.71689, test loss = 0.67424, running test loss = 0.70420, time: 15.55 sec\n",
      "epoch : 964,train loss = 0.71625, test loss = 0.71898, running test loss = 0.70421, time: 15.54 sec\n",
      "epoch : 965,train loss = 0.69121, test loss = 0.64746, running test loss = 0.70415, time: 15.55 sec\n",
      "epoch : 966,train loss = 0.68327, test loss = 0.74233, running test loss = 0.70419, time: 15.55 sec\n",
      "epoch : 967,train loss = 0.76400, test loss = 0.62937, running test loss = 0.70412, time: 15.54 sec\n",
      "epoch : 968,train loss = 0.72284, test loss = 0.67360, running test loss = 0.70409, time: 15.55 sec\n",
      "epoch : 969,train loss = 0.67701, test loss = 0.63942, running test loss = 0.70402, time: 15.55 sec\n",
      "epoch : 970,train loss = 0.67449, test loss = 0.76527, running test loss = 0.70408, time: 15.55 sec\n",
      "epoch : 971,train loss = 0.71721, test loss = 0.74013, running test loss = 0.70412, time: 15.54 sec\n",
      "epoch : 972,train loss = 0.70834, test loss = 0.74107, running test loss = 0.70416, time: 15.59 sec\n",
      "epoch : 973,train loss = 0.69615, test loss = 0.72753, running test loss = 0.70418, time: 15.58 sec\n",
      "epoch : 974,train loss = 0.68600, test loss = 0.69755, running test loss = 0.70417, time: 15.58 sec\n",
      "epoch : 975,train loss = 0.72252, test loss = 0.76220, running test loss = 0.70423, time: 15.60 sec\n",
      "epoch : 976,train loss = 0.71674, test loss = 0.64208, running test loss = 0.70417, time: 15.60 sec\n",
      "epoch : 977,train loss = 0.68119, test loss = 0.64234, running test loss = 0.70411, time: 15.59 sec\n",
      "epoch : 978,train loss = 0.69487, test loss = 0.72473, running test loss = 0.70413, time: 15.58 sec\n",
      "epoch : 979,train loss = 0.70301, test loss = 0.74429, running test loss = 0.70417, time: 15.58 sec\n",
      "epoch : 980,train loss = 0.69199, test loss = 0.72759, running test loss = 0.70419, time: 15.58 sec\n",
      "epoch : 981,train loss = 0.67529, test loss = 0.64159, running test loss = 0.70413, time: 15.59 sec\n",
      "epoch : 982,train loss = 0.69510, test loss = 0.68931, running test loss = 0.70411, time: 15.59 sec\n",
      "epoch : 983,train loss = 0.66756, test loss = 0.69250, running test loss = 0.70410, time: 15.59 sec\n",
      "epoch : 984,train loss = 0.73329, test loss = 0.69641, running test loss = 0.70409, time: 15.58 sec\n",
      "epoch : 985,train loss = 0.66273, test loss = 0.66760, running test loss = 0.70406, time: 15.58 sec\n",
      "epoch : 986,train loss = 0.69123, test loss = 0.66392, running test loss = 0.70402, time: 15.59 sec\n",
      "epoch : 987,train loss = 0.68833, test loss = 0.63689, running test loss = 0.70395, time: 15.59 sec\n",
      "epoch : 988,train loss = 0.68678, test loss = 0.73254, running test loss = 0.70398, time: 15.58 sec\n",
      "epoch : 989,train loss = 0.67686, test loss = 0.55170, running test loss = 0.70382, time: 15.58 sec\n",
      "epoch : 990,train loss = 0.75599, test loss = 0.78318, running test loss = 0.70390, time: 15.59 sec\n",
      "epoch : 991,train loss = 0.69511, test loss = 0.71102, running test loss = 0.70391, time: 15.59 sec\n",
      "epoch : 992,train loss = 0.69929, test loss = 0.67761, running test loss = 0.70388, time: 15.58 sec\n",
      "epoch : 993,train loss = 0.69788, test loss = 0.61332, running test loss = 0.70379, time: 15.58 sec\n",
      "epoch : 994,train loss = 0.69125, test loss = 0.72738, running test loss = 0.70382, time: 15.57 sec\n",
      "epoch : 995,train loss = 0.72846, test loss = 0.69273, running test loss = 0.70381, time: 15.57 sec\n",
      "epoch : 996,train loss = 0.67933, test loss = 0.67704, running test loss = 0.70378, time: 15.58 sec\n",
      "epoch : 997,train loss = 0.69108, test loss = 0.69067, running test loss = 0.70377, time: 15.58 sec\n",
      "epoch : 998,train loss = 0.65678, test loss = 0.66526, running test loss = 0.70373, time: 15.59 sec\n",
      "epoch : 999,train loss = 0.70830, test loss = 0.67406, running test loss = 0.70370, time: 15.58 sec\n"
     ]
    }
   ],
   "source": [
    "running_test_loss = 0.0\n",
    "for run in range(epoch):\n",
    "    start = time.time()\n",
    "    \n",
    "    #Training\n",
    "    sampler.train()\n",
    "    sample.train()\n",
    "    train_loss = 0.0\n",
    "    for ind, data in enumerate(train_loader):\n",
    "        optimizer_sampler.zero_grad()\n",
    "        img, label = data\n",
    "        params, mean, logvar = sampler(img.cuda())\n",
    "        sample.set_params(params)\n",
    "        output = sample(img.cuda())\n",
    "        loss = multi_crit(output, label.cuda(), mean, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer_sampler.step()\n",
    "    train_loss /= (len(train_loader) * 256)\n",
    "    \n",
    "    #Test\n",
    "    sampler.eval()\n",
    "    sample.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for ind, data in enumerate(test_loader):\n",
    "            img, label = data\n",
    "            params, mean, logvar = sampler(img.cuda())\n",
    "            sample.set_params(params)\n",
    "            output = sample(img.cuda())\n",
    "            test_loss += multi_crit(output, label.cuda(), mean, logvar)\n",
    "        test_loss /= (len(test_loader) * 256)\n",
    "        running_test_loss += test_loss\n",
    "    print(\"epoch : %d,train loss = %5.5f, test loss = %5.5f, running test loss = %5.5f, time: %.2f sec\"\n",
    "          %(run, train_loss, test_loss, running_test_loss/(run + 1), time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
